{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associating and Joining Trajectories\n",
    "\n",
    "This relies on the output of [./vectorized_filter.ipynb](./vectorized_filter.ipynb) -> [./lane_classification.ipynb](./lane_classification.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# find the root of the project\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "while not ROOT.joinpath(\".git\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "# add the root to the python path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import polars as pl\n",
    "from pomegranate.distributions import Normal\n",
    "from pomegranate.gmm import GeneralMixtureModel\n",
    "\n",
    "\n",
    "# load the environment variables\n",
    "dotenv.load_dotenv(ROOT.joinpath(\".env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = pl.scan_parquet(\n",
    "    ROOT.joinpath(\"notebooks/clean_workflow/data/imm_filtered_lanes.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pl.scan_parquet(\n",
    "    ROOT.joinpath(\"notebooks/clean_workflow/data/all_working_processed_1Lane.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_df = pl.scan_parquet(\n",
    "    ROOT.joinpath(\"notebooks/clean_workflow/data/offsets.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in the IP Address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "radar_df = (\n",
    "    radar_df\n",
    "    # .fetch(1_000_000)\n",
    "    .lazy()\n",
    "    .sort([\"epoch_time\", \"object_id\"])\n",
    "    .with_columns(\n",
    "        pl.struct([\"epoch_time\", \"object_id\"]).hash(42).alias(\"epoch_object_hash\")\n",
    "    )\n",
    "    .join(\n",
    "        raw_df.select(\n",
    "            list(set(raw_df.columns).difference(set(radar_df.columns)))\n",
    "            + [\"object_id\", \"epoch_time\"]\n",
    "        ).with_columns(\n",
    "            pl.struct([\"epoch_time\", \"object_id\"]).hash(42).alias(\"epoch_object_hash\")\n",
    "        ),\n",
    "        on=\"epoch_object_hash\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(list(set(raw_df.columns).difference(set(radar_df.columns))))\n",
    "        .forward_fill()\n",
    "        .over(\"object_id\"),\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df[\"object_id\"].n_unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Positional Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = (\n",
    "    radar_df\n",
    "    # .tail(1_000_000)\n",
    "    .lazy()\n",
    "    # add in the positional correction\n",
    "    .join(\n",
    "        correction_df.select([\"ip\", \"lane\", \"correction\"]),\n",
    "        on=[\"ip\", \"lane\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"s_filt\") + pl.col(\"correction\").fill_null(0)).alias(\"s_filt\")\n",
    "    )\n",
    "    # find whether the vehicle is going towards or away from the radar\n",
    "    .with_columns(\n",
    "        (pl.col(\"f32_positionX_m\") ** 2 + pl.col(\"f32_positionY_m\") ** 2)\n",
    "        .sqrt()\n",
    "        .alias(\"distance\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        (\n",
    "            (pl.col(\"distance\").diff() <= 0)\n",
    "            .backward_fill()\n",
    "            .over(\"object_id\")\n",
    "            .alias(\"towards_radar\")\n",
    "        )\n",
    "    )\n",
    "    # correct the length estimates\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"f32_distanceToFront_m\") * pl.col(\"s_angle_diff\").cos()).alias(\n",
    "                \"distanceToFront_s\"\n",
    "            ),\n",
    "            (pl.col(\"f32_distanceToBack_m\") * pl.col(\"s_angle_diff\").cos()).alias(\n",
    "                \"distanceToBack_s\"\n",
    "            ),\n",
    "            # do the vehicle length\n",
    "            (pl.col(\"f32_length_m\") * pl.col(\"s_angle_diff\").cos()).alias(\"length_s\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        # use the median vehicle length\n",
    "        pl.col(\"length_s\").median().over(\"object_id\").alias(\"median_length_s\")\n",
    "    )\n",
    "    # Make the assumption that the radar picks up the plane of the vehicle closest to the radar\n",
    "    # try to correct for this and get the true centroid of the vehicle\n",
    "    .with_columns(pl.col(\"s_filt\").alias(\"s_centroid\"))\n",
    "    # correct to find the true front and back of the vehicle\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"s_centroid\") + (pl.col(\"median_length_s\") / 2)).alias(\n",
    "                \"backBumper_s\"\n",
    "            ),\n",
    "            (pl.col(\"s_centroid\") - (pl.col(\"median_length_s\") / 2)).alias(\n",
    "                \"frontBumper_s\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Unique Column for Lane - Lane Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = radar_df.with_columns(\n",
    "    pl.struct([\"lane\", \"lane_index\"]).hash().alias(\"lane_hash\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Leader Follower Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.association.pipelines import build_match_df\n",
    "\n",
    "\n",
    "matching_df = build_match_df(\n",
    "    radar_df.select(\n",
    "        [\n",
    "            \"object_id\",\n",
    "            \"epoch_time\",\n",
    "            \"s_centroid\",\n",
    "            \"s_velocity_filt\",\n",
    "            \"lane\",\n",
    "            \"prediction\",\n",
    "            \"lane_hash\",\n",
    "            \"P\",\n",
    "            \"d_filt\",\n",
    "            \"d_velocity_filt\",\n",
    "            \"frontBumper_s\",\n",
    "            \"backBumper_s\",\n",
    "            \"length_s\",\n",
    "        ]\n",
    "    ),\n",
    ").filter(\n",
    "    ~(pl.col(\"prediction\") & pl.col(\"prediction_leader\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Mahalanobis Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filters.fusion import (\n",
    "    mahalanobis_distance,\n",
    "    loglikelihood,\n",
    "    association_loglikelihood_distance,\n",
    ")\n",
    "from scipy.stats import chi2\n",
    "import torch\n",
    "\n",
    "matching_df = (\n",
    "    matching_df.pipe(\n",
    "        mahalanobis_distance,\n",
    "        cutoff=chi2.ppf(0.99, 4),\n",
    "        gpu=True,\n",
    "        batch_size=100_000,\n",
    "    ).pipe(\n",
    "        association_loglikelihood_distance,\n",
    "        gpu=True,\n",
    "    )\n",
    "    # p(a = b) = 1 - p(a <> b) = 1 - (p(birth) + p(error) + p())\n",
    "    # If I use a validation gate, then I have to normalize by the area of the gate\n",
    "    # The potenital of using a complicated birth model here is\n",
    "    # for now just rely on the gate\n",
    "    # .pipe(loglikelihood, gpu=True)\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Headways and Find the Middle of Leader-Follower Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.association.pipelines import calculate_match_indexes, pipe_gate_headway_calc\n",
    "\n",
    "matching_df = matching_df.filter(\n",
    "    (pl.col(\"s_velocity_filt\").abs() > 2) & (pl.col(\"s_velocity_filt_leader\").abs() > 2)\n",
    ").pipe(\n",
    "    calculate_match_indexes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_matches = (\n",
    "    matching_df.pipe(pipe_gate_headway_calc, alpha=0.1)\n",
    "    # .filter(\n",
    "    #     (pl.col(\"inside_gate\") > 0.5)\n",
    "    # )\n",
    "    .sort(\"epoch_time\")\n",
    "    .unnest(\"pair\")\n",
    "    .with_row_count()\n",
    "    .join(\n",
    "        radar_df.select([\"object_id\", \"epoch_time\"])\n",
    "        .group_by(\"object_id\")\n",
    "        .agg(pl.col(\"epoch_time\").max().alias(\"epoch_time_max\")),\n",
    "        on=\"object_id\",\n",
    "    )\n",
    "    .join(\n",
    "        radar_df.select([\"object_id\", \"epoch_time\"])\n",
    "        .group_by(\"object_id\")\n",
    "        .agg(pl.col(\"epoch_time\").max().alias(\"epoch_time_max_leader\")),\n",
    "        left_on=\"leader\",\n",
    "        right_on=\"object_id\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_matches.filter(pl.col(\"association_distance_filt\") < 10).to_pandas()[\n",
    "    \"association_distance_filt\"\n",
    "].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi.ppf(0.99, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_rows = (\n",
    "    valid_matches.melt(\n",
    "        id_vars=[\n",
    "            \"epoch_time\",\n",
    "            \"row_nr\",\n",
    "            \"prediction\",\n",
    "            \"prediction_leader\",\n",
    "            \"epoch_time_max\",\n",
    "            \"epoch_time_max_leader\",\n",
    "            \"association_distance_filt\",\n",
    "        ],\n",
    "        value_vars=[\n",
    "            \"object_id\",\n",
    "            \"leader\",\n",
    "        ],\n",
    "    )\n",
    "    .filter((pl.col(\"association_distance_filt\") < chi.ppf(0.9875, 4)))\n",
    "    .sort(\"value\", \"epoch_time\")\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"variable\") == \"object_id\")\n",
    "        .then(pl.col(\"prediction\"))\n",
    "        .otherwise(pl.col(\"prediction_leader\"))\n",
    "        .alias(\"prediction\"),\n",
    "        pl.when(pl.col(\"variable\") == \"object_id\")\n",
    "        .then(pl.col(\"epoch_time_max\"))\n",
    "        .otherwise(pl.col(\"epoch_time_max_leader\"))\n",
    "        .alias(\"my_end_time\"),\n",
    "        pl.when(pl.col(\"variable\") == \"object_id\")\n",
    "        .then(pl.col(\"epoch_time_max_leader\"))\n",
    "        .otherwise(pl.col(\"epoch_time_max\"))\n",
    "        .alias(\"other_end_time\"),\n",
    "    )\n",
    "    .drop([\"epoch_time_max\", \"epoch_time_max_leader\", \"prediction_leader\", \"variable\"])\n",
    "    .with_columns(\n",
    "        pl.col(\"prediction\").cumsum().over(\"value\").alias(\"prediction_count\"),\n",
    "        pl.col(\"other_end_time\")\n",
    "        .filter(~pl.col(\"prediction\"))\n",
    "        .max()\n",
    "        .over(\"value\")\n",
    "        .alias(\"other_end_time_max\"),\n",
    "    )\n",
    "    .filter((pl.col(\"prediction_count\") <= 1))\n",
    "    .filter(pl.col(\"row_nr\").count().over(\"row_nr\") > 1)\n",
    ")\n",
    "\n",
    "keep_rows.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_matches = valid_matches.filter(\n",
    "    pl.col(\"row_nr\").is_in(keep_rows[\"row_nr\"].unique())\n",
    ").drop(\"row_nr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Graph of Connected Vehicles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.association.pipelines import create_vehicle_ids\n",
    "\n",
    "joined_df = radar_df.pipe(\n",
    "    create_vehicle_ids,\n",
    "    match_df=valid_matches,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark Vehicle Group Ends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting.time_space import plot_time_space\n",
    "from datetime import timedelta\n",
    "from src.radar import Filtering\n",
    "\n",
    "\n",
    "# get a 10 minute window\n",
    "plot_df = joined_df.filter(\n",
    "    pl.col(\"epoch_time\").is_between(\n",
    "        joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=40),\n",
    "        joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=45),\n",
    "    )\n",
    "    & (pl.col(\"lane\").str.contains(\"WBL1\"))\n",
    "    & (pl.col(\"lane_index\") == 1)\n",
    "    # (pl.col(\"vehicle_id\") == pl.lit(15420721423209556182))\n",
    "    # pl.col('object_id').is_in([254, 147,])\n",
    ").pipe(Filtering.add_cst_timezone)\n",
    "\n",
    "fig = plot_time_space(\n",
    "    plot_df,\n",
    "    hoverdata=\"object_id\",\n",
    "    vehicle_col=\"vehicle_id\",\n",
    "    s_col=\"s_centroid\",\n",
    "    markers=True,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.group_by([\"vehicle_id\", \"epoch_time\"]).count()[\n",
    "    \"count\"\n",
    "].value_counts().with_columns(\n",
    "    [(pl.col(\"counts\") / pl.col(\"counts\").sum()).alias(\"percent\")]\n",
    ").sort(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a 4D Data Frame of Vehicle States\n",
    "\n",
    "- The dimensions are Time\n",
    "- The Vehicle\n",
    "- Measurements 1-3\n",
    "- X dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ci_df = joined_df.filter(~pl.col(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_df = (\n",
    "    joined_df.lazy()\n",
    "    # .filter(~pl.col(\"prediction\"))\n",
    ")\n",
    "\n",
    "ci_df = (\n",
    "    ci_df.sort([\"vehicle_id\", \"epoch_time\"])\n",
    "    # .set_sorted([\"vehicle_id\", \"epoch_time\"])\n",
    "    .join(\n",
    "        ci_df.select([\"vehicle_id\", \"epoch_time\"])\n",
    "        .unique()\n",
    "        .sort([\"vehicle_id\", \"epoch_time\"])\n",
    "        .with_columns(\n",
    "            (pl.col(\"epoch_time\").cumcount()).over(\"vehicle_id\").alias(\"time_index\")\n",
    "        ),\n",
    "        on=[\"vehicle_id\", \"epoch_time\"],\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"epoch_time\").first().over(\"vehicle_id\").alias(\"vehicle_start_time\"),\n",
    "    )\n",
    "    .sort(\n",
    "        [\n",
    "            \"prediction\",\n",
    "            \"vehicle_start_time\",\n",
    "        ]\n",
    "    )\n",
    "    # .set_sorted([\"vehicle_id\", \"vehicle_start_time\", \"epoch_time\"])\n",
    "    .with_columns(\n",
    "        pl.col(\"object_id\")\n",
    "        .cumcount()\n",
    "        .over([\"vehicle_id\", \"time_index\"])\n",
    "        .alias(\"vehicle_time_index_int\")\n",
    "    )\n",
    "    .filter(pl.col(\"vehicle_time_index_int\") < 3)\n",
    "    .sort(\"epoch_time\")\n",
    "    # .set_sorted(\"epoch_time\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"epoch_time\").diff() / 1000)\n",
    "        .cast(float)\n",
    "        .over(\n",
    "            \"object_id\",\n",
    "        )\n",
    "        .fill_null(0)\n",
    "        .alias(\"time_diff\")\n",
    "    )\n",
    "    .drop([\"vehicle_start_time\", \"time_ind\", \"vehicle_ind\"])\n",
    "    .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join using CI & then RTS Smooth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filters.fusion import batch_join, rts_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = batch_join(ci_df, method=\"CI\", batch_size=5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = rts_smooth(merged_df, gpu=True, batch_size=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting.time_space import plot_time_space\n",
    "from datetime import timedelta\n",
    "from src.radar import Filtering\n",
    "\n",
    "\n",
    "# get a 10 minute window\n",
    "plot_df = (\n",
    "    merged_df.filter(\n",
    "        pl.col(\"epoch_time\").is_between(\n",
    "            joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=40),\n",
    "            joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=45),\n",
    "        )\n",
    "        & (pl.col(\"lane\").str.contains(\"WBL1\"))\n",
    "        & (pl.col(\"lane_index\") == 1)\n",
    "        # & (pl.col(\"vehicle_id\").is_in(['563']))\n",
    "        # pl.col('object_id').is_in([254, 147,])\n",
    "    )\n",
    "    .pipe(Filtering.add_cst_timezone)\n",
    "    .sort(\"epoch_time\")\n",
    ")\n",
    "\n",
    "fig = plot_time_space(\n",
    "    plot_df,\n",
    "    hoverdata=\"vehicle_id\",\n",
    "    vehicle_col=\"vehicle_id\",\n",
    "    markers=True,\n",
    "    s_col=\"s_smooth\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "veh = 4759\n",
    "\n",
    "plot_df = merged_df.filter(pl.col(\"vehicle_id\") == veh)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=plot_df[\"epoch_time\"],\n",
    "        y=plot_df[\"s_velocity_smooth\"] * -1,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"green\", size=2),\n",
    "        name=\"s_velocity_smooth\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=plot_df[\"epoch_time\"],\n",
    "        y=plot_df[\"ci_s_velocity\"] * -1,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"blue\", size=2),\n",
    "        name=\"s_velocity_smooth\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "individual_traj = joined_df.filter(pl.col(\"vehicle_id\") == veh).sort(\"epoch_time\")\n",
    "\n",
    "for v, v_df in individual_traj.group_by(\"object_id\"):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=v_df[\"epoch_time\"],\n",
    "            y=v_df[\"s_velocity\"] * -1,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"red\", size=2),\n",
    "            name=f\"{v}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "veh = 5100\n",
    "\n",
    "plot_df = merged_df.filter(pl.col(\"vehicle_id\") == veh)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=plot_df[\"epoch_time\"],\n",
    "        y=plot_df[\"s_smooth\"] * -1,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"blue\", size=2),\n",
    "        name=\"s_velocity_smooth\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "individual_traj = joined_df.filter(pl.col(\"vehicle_id\") == veh).sort(\"epoch_time\")\n",
    "\n",
    "for v, v_df in individual_traj.group_by(\"object_id\"):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=v_df[\"epoch_time\"],\n",
    "            y=v_df[\"s_filt\"] * -1,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=\"red\", size=2),\n",
    "            name=f\"{v}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# reverse the y axis\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Filtered Trajectory Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = merged_df.select(\n",
    "    [\"vehicle_id\", \"lane\", \"lane_index\", \"s_smooth\", \"s_velocity_smooth\", \"epoch_time\"]\n",
    ").join(\n",
    "    joined_df.group_by([\"vehicle_id\", \"lane\", \"lane_index\", \"epoch_time\"]).agg(\n",
    "        pl.col(\"length_s\").mean().alias(\"length_s\"),\n",
    "    ),\n",
    "    on=[\"vehicle_id\", \"lane\", \"lane_index\", \"epoch_time\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df.sort([\"vehicle_id\", \"epoch_time\"]).write_parquet(\n",
    "    ROOT.joinpath(\"notebooks/clean_workflow/data/merged_trajectories.parquet\"),\n",
    "    compression_level=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.select([\"object_id\", \"vehicle_id\"]).unique().write_parquet(\n",
    "    ROOT.joinpath(\"notebooks/clean_workflow/data/vehicle_id_map.parquet\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
