{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMM Filtering all Trajectories with a Vectorized Approach\n",
    "\n",
    "The input file for this comes from [../radar/lane_classification.ipynb](../radar/lane_classification.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# find the root of the project\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "while not ROOT.joinpath(\".git\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "# add the root to the python path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "# load the environment variables\n",
    "dotenv.load_dotenv(ROOT.joinpath(\".env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the _Already Processed_ Radar Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.radar import Filtering\n",
    "\n",
    "\n",
    "f = Filtering(\n",
    "    network_boundary_path=ROOT / \"geo_data\" / \"network_outline.geojson\",\n",
    "    radar_location_path=ROOT / \"geo_data\" / \"calibrated_origins.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "radar_df = pl.scan_parquet(\n",
    "    ROOT\n",
    "    / \"notebooks\"\n",
    "    / \"clean_workflow\"\n",
    "    / \"data\"\n",
    "    / \"all_working_processed_1Lane.parquet\",\n",
    ")\n",
    "\n",
    "# radar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "\n",
    "radar_df = (\n",
    "    radar_df\n",
    "    # .fetch(10_000_000)\n",
    "    .lazy()\n",
    "    .filter(\n",
    "        (pl.col(\"s\").is_not_null()) & (pl.col(\"lane\").is_not_null())\n",
    "        # & (pl.col(\"epoch_time\") < (pl.col(\"epoch_time\").min() + timedelta(minutes=120)))\n",
    "    )\n",
    "    .sort(\"epoch_time\", descending=False)\n",
    "    .set_sorted([\"epoch_time\"])\n",
    "    ## ---------------------------------------------------\n",
    "    ## Add Lane Group\n",
    "    ## ---------------------------------------------------\n",
    "    # .pipe(f.add_lane_group)\n",
    "    # ---------------------------------------------------\n",
    "    ## Remove all of the Radar's Internal Predictions (only the end)\n",
    "    ## ---------------------------------------------------\n",
    "    # .filter(pl.col(\"ui16_predictionCount\") < 1)\n",
    "    .with_columns(\n",
    "        [\n",
    "            # Trim off the ends of trajectoriers where the radar has done predictions.\n",
    "            # This can be done by reversing the cummulative count of predictions and comparing to the reverse cummulative count\n",
    "            # They are equal until the last measure data point\n",
    "            (\n",
    "                pl.col(\"ui16_predictionCount\").count()\n",
    "                - pl.col(\"ui16_predictionCount\").cumcount()\n",
    "            )\n",
    "            .over(\"object_id\")\n",
    "            .alias(\"cumcount\"),\n",
    "            (pl.col(\"ui16_predictionCount\") > 0)\n",
    "            .reverse()\n",
    "            .cumsum()\n",
    "            .reverse()\n",
    "            .over(\"object_id\")\n",
    "            .alias(\"reverse_cumcount\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"cumcount\") != pl.col(\"reverse_cumcount\")).alias(\"keep\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"keep\"))\n",
    "    .drop([\"keep\", \"cumcount\", \"reverse_cumcount\"])\n",
    "    # .with_columns(\n",
    "    #     [\n",
    "    #         (pl.col(\"epoch_time\").diff() / 1000)\n",
    "    #         .backward_fill()\n",
    "    #         .over([\"object_id\", \"lane\"])\n",
    "    #         .alias(\"time_diff\"),\n",
    "    #     ]\n",
    "    # )\n",
    "    ## ---------------------------------------------------\n",
    "    ## Calculating the S and D components of the velocity\n",
    "    ## ---------------------------------------------------\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"direction\") - pl.col(\"s_angle\")).sin().alias(\"sin\"),\n",
    "            (pl.col(\"direction\") - pl.col(\"s_angle\")).cos().alias(\"cos\"),\n",
    "        ]\n",
    "    )\n",
    "    .pipe(f.atan2, x_col=\"cos\", y_col=\"sin\", out_col=\"s_angle_diff\", normalize=False)\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"s_angle_diff\").abs() > np.deg2rad(30))\n",
    "        .then(None)\n",
    "        .otherwise(pl.col(\"s_angle_diff\"))\n",
    "        .alias(\"s_angle_diff\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"s_angle_diff\")\n",
    "            .interpolate()\n",
    "            .over([\"object_id\", \"lane\"])\n",
    "            .alias(\"s_angle_diff\")\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"s_angle_diff\").is_not_null())\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"f32_velocityInDir_mps\") * pl.col(\"s_angle_diff\").cos()).alias(\n",
    "                \"s_velocity\"\n",
    "            )\n",
    "            * -1,\n",
    "            (\n",
    "                pl.col(\"f32_velocityInDir_mps\")\n",
    "                * pl.col(\"s_angle_diff\").sin()\n",
    "                * pl.when(pl.col(\"lane\").str.contains(\"W\")).then(-1).otherwise(1)\n",
    "            ).alias(\"d_velocity\"),\n",
    "        ]\n",
    "    )\n",
    "    ## ---------------------------------------------------\n",
    "    ## Map the distance to the front and back of the\n",
    "    ##          vehicle to the S dimension\n",
    "    ## ---------------------------------------------------\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"f32_distanceToFront_m\") * pl.col(\"s_angle_diff\").cos()).alias(\n",
    "                \"distanceToFront_s\"\n",
    "            ),\n",
    "            (pl.col(\"f32_distanceToBack_m\") * pl.col(\"s_angle_diff\").cos()).alias(\n",
    "                \"distanceToBack_s\"\n",
    "            ),\n",
    "            # do the vehicle length\n",
    "            (pl.col(\"f32_length_m\") * pl.col(\"s_angle_diff\").cos()).alias(\"length_s\"),\n",
    "        ]\n",
    "    )\n",
    "    ## ---------------------------------------------------\n",
    "    ## Making a Z (measurement) vector for the Kalman Filter (+ time)\n",
    "    ## ---------------------------------------------------\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.concat_list(\n",
    "                [\n",
    "                    pl.col(\"s\"),\n",
    "                    pl.col(\"s_velocity\"),\n",
    "                    pl.col(\"min_d\"),\n",
    "                    pl.col(\"d_velocity\"),\n",
    "                ]\n",
    "            ).alias(\"z\")\n",
    "        ]\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df[\"epoch_time\"].min(), radar_df[\"epoch_time\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Short Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = (\n",
    "    radar_df.with_columns(\n",
    "        ((pl.col(\"epoch_time\").max() - pl.col(\"epoch_time\").min()) / 1000)\n",
    "        .over(\"object_id\")\n",
    "        .alias(\"lane_time_diff\"),\n",
    "        (pl.col(\"s\").max() - pl.col(\"s\").min())\n",
    "        .abs()\n",
    "        .over(\"object_id\")\n",
    "        .alias(\"lane_distance\"),\n",
    "    )\n",
    "    # gotta be on the lane for 5 seconds & travel for 10 meters\n",
    "    .filter((pl.col(\"lane_time_diff\") > 5) & (pl.col(\"lane_distance\") > 5))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Lane Trajectories As Needing Extension or Not\n",
    "\n",
    "Need an extension if the vehicle ends near a lane center. Otherwise we assume that it has left the netwerk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aka do the ends need a match or not\n",
    "radar_df = (\n",
    "    radar_df.lazy()\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"epoch_time\")\n",
    "            .max()\n",
    "            .over(\n",
    "                [\n",
    "                    \"object_id\",\n",
    "                ]\n",
    "            )\n",
    "            .alias(\"max_time_vehicle\"),\n",
    "            pl.col(\"epoch_time\")\n",
    "            .min()\n",
    "            .over(\n",
    "                [\n",
    "                    \"object_id\",\n",
    "                ]\n",
    "            )\n",
    "            .alias(\"min_time_vehicle\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"epoch_time\") == pl.col(\"max_time_vehicle\"))\n",
    "            .over([\"object_id\"])\n",
    "            .alias(\"is_end\"),\n",
    "            (pl.col(\"epoch_time\") == pl.col(\"min_time_vehicle\"))\n",
    "            .over([\"object_id\"])\n",
    "            .alias(\"is_start\"),\n",
    "            pl.col(\"min_d\").last().over([\"object_id\"]).alias(\"last_d\"),\n",
    "        ]\n",
    "    )\n",
    "    .drop([\"max_time_vehicle\", \"min_time_vehicle\"])\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = (\n",
    "    radar_df.lazy()\n",
    "    .sort(\n",
    "        by=[\"epoch_time\"],\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"is_end\").any() & pl.col(\"last_d\").is_between(-2, 5))\n",
    "        .over([\"object_id\"])\n",
    "        .alias(\"extend\")\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling and Building Prediction onto Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extend_df = (\n",
    "    radar_df.lazy()\n",
    "    .group_by(\n",
    "        [\n",
    "            \"object_id\",\n",
    "        ]\n",
    "    )\n",
    "    .agg([pl.col(\"extend\").any().alias(\"extend_me\")])\n",
    "    .filter(pl.col(\"extend_me\"))\n",
    "    # .with_columns(pl.lit(0.1).alias(\"timestep\"))\n",
    "    # .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is miles faster than upsample\n",
    "radar_df = (\n",
    "    radar_df.lazy()\n",
    "    .join(\n",
    "        extend_df,\n",
    "        on=[\"object_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.lit(0.1).alias(\"timestep\"),\n",
    "    )\n",
    "    .select(\n",
    "        pl.when(pl.col(\"extend_me\"))\n",
    "        .then(\n",
    "            ((pl.col(\"lane_time_diff\") / 0.1) + 4 / 0.1).cast(pl.UInt32),\n",
    "        )\n",
    "        .otherwise(\n",
    "            (pl.col(\"lane_time_diff\") / 0.1).cast(pl.UInt32),\n",
    "        )\n",
    "        .alias(\"time_count\"),\n",
    "        pl.col(\"epoch_time\"),\n",
    "        pl.col(\"object_id\"),\n",
    "        pl.col(\"timestep\"),\n",
    "        pl.col(\"extend_me\"),\n",
    "    )\n",
    "    .group_by(\"object_id\")\n",
    "    .agg(\n",
    "        (\n",
    "            pl.col(\"timestep\").first().repeat_by(pl.col(\"time_count\").first()).cumsum()\n",
    "            - pl.col(\"timestep\").first()\n",
    "        ).alias(\"timestep\"),\n",
    "        pl.col(\"epoch_time\").first(),\n",
    "        pl.col(\"epoch_time\").last().alias(\"last_epoch_time\"),\n",
    "        pl.col(\"extend_me\").any().alias(\"extend_me\"),\n",
    "    )\n",
    "    .explode(\"timestep\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"epoch_time\") + pl.col(\"timestep\") * 1e3)\n",
    "        .cast(pl.Datetime(time_unit=\"ms\", time_zone=\"UTC\"))\n",
    "        .alias(\"epoch_time\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.struct([\"object_id\", \"epoch_time\"]).hash().alias(\"join_key\"),\n",
    "        (\n",
    "            (pl.col(\"epoch_time\") > pl.col(\"last_epoch_time\")) & pl.col(\"extend_me\")\n",
    "        ).alias(\"prediction\"),\n",
    "    )\n",
    "    .drop(\n",
    "        [\n",
    "            \"time_count\",\n",
    "            \"object_id\",\n",
    "            \"timestep\",\n",
    "            \"extend_me\",\n",
    "        ]\n",
    "    )\n",
    "    .join(\n",
    "        radar_df.lazy()\n",
    "        .with_columns(pl.struct([\"object_id\", \"epoch_time\"]).hash().alias(\"join_key\"))\n",
    "        .drop([\"epoch_time\"])\n",
    "        .with_columns(pl.lit(False).alias(\"missing_data\")),\n",
    "        on=\"join_key\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"prediction\").fill_null(False).alias(\"prediction\"),\n",
    "        pl.col(\"missing_data\").fill_null(True).alias(\"missing_data\"),\n",
    "        pl.col(\"z\").fill_null(pl.lit(None)).alias(\"z\"),\n",
    "    )\n",
    "    .with_columns(pl.col(set(radar_df.columns) ^ {\"prediction\", \"z\"}).forward_fill())\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_df.filter(\n",
    "#     pl.col(\"missing_data\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # radar_df = radar_df.with_columns(pl.lit(False).alias(\"prediction\"))\n",
    "# tmp_df = (\n",
    "#     radar_df.lazy()\n",
    "#     .group_by(\n",
    "#         [\n",
    "#             \"object_id\",\n",
    "#             \"lane\",\n",
    "#         ]\n",
    "#     )\n",
    "#     .agg([pl.col(\"extend\").any().alias(\"extend_me\"), pl.all().last()])\n",
    "#     .filter(pl.col(\"extend_me\"))\n",
    "#     .with_columns(pl.lit(0.1).alias(\"timestep\"))\n",
    "#     .group_by(\"object_id\")\n",
    "#     .agg(\n",
    "#         pl.col(\"timestep\").repeat_by(int(5 / 0.1) + 1).cumsum()\n",
    "#         - pl.col(\"timestep\").first(),\n",
    "#         pl.col(\"epoch_time\").first(),\n",
    "#     )\n",
    "#     .explode(\"timestep\")\n",
    "#     .with_columns(\n",
    "#         (pl.col(\"epoch_time\") + (pl.col(\"timestep\") * 1000).cast(int))\n",
    "#         .cast(pl.Datetime(\"ms\", time_zone=\"UTC\"))\n",
    "#         .alias(\"epoch_time\"),\n",
    "#         pl.lit(True).alias(\"prediction\"),\n",
    "#     )\n",
    "#     .select([\"object_id\", \"epoch_time\", \"prediction\"])\n",
    "#     .join(radar_df.lazy(), on=[\"object_id\", \"epoch_time\"], how=\"left\")\n",
    "#     # if we sort we don't have to window the next function\n",
    "#     .sort(\"object_id\")\n",
    "#     .with_columns(\n",
    "#         [\n",
    "#             pl.all().forward_fill(),\n",
    "#         ]\n",
    "#     )\n",
    "#     .with_columns(\n",
    "#         pl.lit(None, dtype=pl.List(pl.Float64())).alias(\"z\"),\n",
    "#     )\n",
    "#     .collect()\n",
    "# )\n",
    "\n",
    "\n",
    "# radar_df = (\n",
    "#     tmp_df.extend(\n",
    "#         radar_df.with_columns(pl.lit(False).alias(\"prediction\")).select(tmp_df.columns)\n",
    "#     )\n",
    "#     .unique([\"object_id\", \"epoch_time\"])\n",
    "#     .sort(\"epoch_time\")\n",
    "#     .set_sorted([\"epoch_time\"])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = radar_df.sort(\n",
    "    by=[\"epoch_time\"],\n",
    ").with_columns(\n",
    "    [\n",
    "        (pl.col(\"epoch_time\").diff() / 1000)\n",
    "        .backward_fill(1)\n",
    "        .over(\n",
    "            \"object_id\",\n",
    "        )\n",
    "        .fill_null(0)\n",
    "        .alias(\"time_diff\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "# 1. split trajectories that have dt > 4? seconds into different trajectories (done by changing the object id)\n",
    "radar_df = (\n",
    "    radar_df.with_columns(\n",
    "        (pl.col(\"time_diff\") > 4).alias(\"split\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"split\").cumsum().over([\"object_id\", \"lane\"]).alias(\"trajectory_id\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.struct(\n",
    "            (pl.col(\"object_id\"), pl.col(\"trajectory_id\")),\n",
    "        )\n",
    "        .hash()\n",
    "        .alias(\"kalman_id\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (\n",
    "    radar_df.lazy()\n",
    "    .select(\n",
    "        [\n",
    "            \"kalman_id\",\n",
    "            \"epoch_time\",\n",
    "            \"z\",\n",
    "            \"time_diff\",\n",
    "            \"prediction\",\n",
    "            \"missing_data\",\n",
    "            \"lane\",\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"epoch_time\").cumcount().over(\"kalman_id\").alias(\"time_ind\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# filter out trajectories that just have one point\n",
    "test_df = test_df.filter(\n",
    "    pl.col(\"kalman_id\").is_in(\n",
    "        test_df.group_by(\"kalman_id\")\n",
    "        .agg(pl.col(\"time_ind\").max())\n",
    "        .filter(pl.col(\"time_ind\") > 10)\n",
    "        .select(\"kalman_id\")\n",
    "        .collect()[\"kalman_id\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "test_df = (\n",
    "    test_df.join(\n",
    "        test_df.group_by(\"kalman_id\").agg(\n",
    "            pl.col(\"time_ind\").max().alias(\"time_ind_max\")\n",
    "        )\n",
    "        # .sort(\"time_ind\")  # turn this on to get an ordered list of trajectories from small to large.\n",
    "        .with_row_count(\"vehicle_ind\"),\n",
    "        on=\"kalman_id\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .sort([\"vehicle_ind\", \"time_ind\"])\n",
    "    .with_columns(pl.col(\"vehicle_ind\").cast(pl.Int32).alias(\"vehicle_ind\"))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Short Trajectories Again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# list gpu devices\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions\", test_df[\"vehicle_ind\"].max(), \"x\", test_df[\"time_ind\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filters.vectorized_kalman import batch_imm_df\n",
    "import gc\n",
    "\n",
    "filt_df = batch_imm_df(\n",
    "    test_df,\n",
    "    filters=(\"CALC\", \"CALK\", \"CVLK\"),\n",
    "    M=np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]),\n",
    "    mu=np.array([0.05, 0.3, 0.65]),\n",
    "    # chunk_size=3_500,\n",
    "    chunk_size=4000,\n",
    "    gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the filtered states to the original data\n",
    "joined_df = (\n",
    "    test_df.with_columns(\n",
    "        [\n",
    "            # # unpack the list of z values\n",
    "            pl.col(\"z\").list.get(0).alias(\"s\"),\n",
    "            pl.col(\"z\").list.get(1).alias(\"s_velocity\"),\n",
    "            pl.col(\"z\").list.get(2).alias(\"d\"),\n",
    "            pl.col(\"z\").list.get(3).alias(\"d_velocity\"),\n",
    "        ]\n",
    "    )\n",
    "    .join(\n",
    "        filt_df.with_columns(pl.col(\"time_ind\").cast(pl.UInt32)),\n",
    "        on=[\"time_ind\", \"vehicle_ind\"],\n",
    "        how=\"inner\",\n",
    "        suffix=\"_filt\",\n",
    "    )\n",
    "    # need these later...\n",
    "    .join(\n",
    "        radar_df.select(\n",
    "            [\"object_id\", \"epoch_time\", \"s_angle_diff\", \"is_end\", \"kalman_id\"]\n",
    "        ),\n",
    "        on=[\"kalman_id\", \"epoch_time\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    .drop([\"kalman_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# from src.filters.kalman2 import kf_filter_lanechange\n",
    "\n",
    "# veh = joined_df['object_id'].sample(1).to_numpy()[0]\n",
    "veh_id = joined_df.filter(pl.col(\"lane\") == \"EBL1\")[\"object_id\"].sample(1)[0]\n",
    "# veh_df = joined_df.filter(pl.col(\"vehicle_ind\") == veh).sort(\"epoch_time\")\n",
    "veh_df = joined_df.filter(pl.col(\"object_id\") == veh_id).sort(\"epoch_time\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3,\n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.02,\n",
    "    subplot_titles=(\n",
    "        f\"Vehicle {veh_id} S\",\n",
    "        f\"Vehicle {veh_id} D\",\n",
    "    ),\n",
    "    # add a secondary y axis to the velocity plots\n",
    "    specs=[\n",
    "        [{\"secondary_y\": True}],\n",
    "        [{\"secondary_y\": True}],\n",
    "        [{\"secondary_y\": False}],\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "colors = {\n",
    "    \"\": \"blue\",\n",
    "    \"_filt\": \"red\",\n",
    "}\n",
    "\n",
    "\n",
    "for df, ext in [(veh_df, \"\"), (veh_df, \"_filt\")]:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=veh_df[\"epoch_time\"],\n",
    "            y=df[f\"s{ext}\"],\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"S{ext}\",\n",
    "            marker_color=colors[ext],\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=veh_df[\"epoch_time\"],\n",
    "            y=df[f\"s_velocity{ext}\"] * -1,\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"S Velocity{ext}\",\n",
    "            marker_color=colors[ext],\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    # add the D dimension\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=veh_df[\"epoch_time\"],\n",
    "            y=df[f\"d{ext}\"],\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"D{ext}\",\n",
    "            marker_color=colors[ext],\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=veh_df[\"epoch_time\"],\n",
    "            y=df[f\"d_velocity{ext}\"],\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"D Velocity{ext}\",\n",
    "            marker_color=colors[ext],\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "\n",
    "for p in [\"mu_CALC\", \"mu_CALK\", \"mu_CVLK\"]:\n",
    "    # plot the probabilities\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=veh_df[\"epoch_time\"],\n",
    "            y=veh_df[p],\n",
    "            mode=\"markers+lines\",\n",
    "            name=p,\n",
    "            # marker_color=\"green\",\n",
    "        ),\n",
    "        row=3,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "\n",
    "# bound the y axis\n",
    "# fig.update_yaxes(range=[-10, 100], row=1, col=1)\n",
    "# fig.update_yaxes(range=[-10, 10], row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Filtered Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.write_parquet(\n",
    "    ROOT / \"notebooks/clean_workflow/data\" / \"imm_filtered.parquet\",\n",
    "    # compression=\"gzip\",\n",
    "    use_pyarrow=True,\n",
    "    compression=\"lz4\",\n",
    "    # parti\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar-trajectories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
