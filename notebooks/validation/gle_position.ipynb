{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using dGPS on GLE to Validate the Fusion Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# find the root of the project\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import polars as pl\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "while not ROOT.joinpath(\".git\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "# add the root to the python path\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "SHOW_FIGS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the GLE Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the GLE\n",
    "\n",
    "GLE_m = 4.93522\n",
    "DIST_2_FRONT_m = 2.91\n",
    "DIST_2_BACK_m = 1.99\n",
    "\n",
    "\n",
    "veh_df = (\n",
    "    pl.read_csv(\n",
    "        ROOT / \"data\" / \"vehicle_drives\" / \"2023-10-31.csv\",\n",
    "    )\n",
    "    .drop(\"\")\n",
    "    .with_columns(\n",
    "        pl.col(\"gps_time\").str.strptime(\n",
    "            dtype=pl.Datetime(\n",
    "                time_unit=\"us\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    .sort(\n",
    "        \"gps_time\",\n",
    "    )\n",
    "    .with_row_count(name=\"seq\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from src.geometry import RoadNetwork\n",
    "\n",
    "\n",
    "network = RoadNetwork(\n",
    "    lane_gdf=gpd.read_file(ROOT / \"data/mainline_lanes.geojson\"),\n",
    "    step_size=0.01,\n",
    ")\n",
    "\n",
    "lane_df = network.df\n",
    "\n",
    "LANE_WIDTH = 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snap the Vehicle Trajectories to Lanes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm\n",
    "\n",
    "x, y, _, _ = utm.from_latlon(\n",
    "    latitude=veh_df[\"lat\"].to_numpy(),\n",
    "    longitude=veh_df[\"lon\"].to_numpy(),\n",
    ")\n",
    "\n",
    "GLE_m = 4.93522\n",
    "DIST_2_FRONT_m = 2.91\n",
    "DIST_2_BACK_m = 1.99\n",
    "\n",
    "\n",
    "veh_df = (\n",
    "    veh_df.sort(\"gps_time\")\n",
    "    .with_columns(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        gps_time=pl.col(\"gps_time\").dt.truncate(\"100ms\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"x\").diff().alias(\"dx\"),\n",
    "        pl.col(\"y\").diff().alias(\"dy\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"dx\").pow(2) + pl.col(\"dy\").pow(2)).sqrt().alias(\"dist\"),\n",
    "        # find the heading\n",
    "        pl.arctan2(pl.col(\"dy\"), pl.col(\"dx\")).alias(\"heading\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"dist\").cum_sum().alias(\"cum_dist\"),\n",
    "        # make the front x/y position\n",
    "        (pl.col(\"x\") + (DIST_2_FRONT_m * pl.col(\"heading\").cos())).alias(\"front_x\"),\n",
    "        (pl.col(\"y\") + (DIST_2_FRONT_m * pl.col(\"heading\").sin())).alias(\"front_y\"),\n",
    "        (pl.col(\"x\") - (DIST_2_BACK_m * pl.col(\"heading\").cos())).alias(\"back_x\"),\n",
    "        (pl.col(\"y\") - (DIST_2_BACK_m * pl.col(\"heading\").sin())).alias(\"back_y\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        #\n",
    "        (\n",
    "            (pl.col(\"cum_dist\").shift(-1) - pl.col(\"cum_dist\").shift(1)).abs()\n",
    "            / (\n",
    "                (\n",
    "                    pl.col(\"gps_time\").shift(-1) - pl.col(\"gps_time\").shift(1)\n",
    "                ).dt.total_milliseconds()\n",
    "                / 1e3\n",
    "            )\n",
    "        ).alias(\"speed\"),\n",
    "    )\n",
    "    .pipe(\n",
    "        network.map_to_lane,\n",
    "        dist_upper_bound=LANE_WIDTH,\n",
    "        utm_x_col=\"x\",\n",
    "        utm_y_col=\"y\",\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col(\"name\").is_not_null(),\n",
    "    )\n",
    "    .rename(\n",
    "        {\n",
    "            \"name\": \"lane\",\n",
    "            \"angle\": \"heading_lane\",\n",
    "        }\n",
    "    )\n",
    "    .with_columns((pl.col(\"heading\") - pl.col(\"heading_lane\")).alias(\"angle_diff\"))\n",
    "    .with_columns(\n",
    "        # find the portion of velocity that is in the direction of the lane\n",
    "        pl.arctan2(pl.col(\"angle_diff\").sin(), pl.col(\"angle_diff\").cos()).alias(\n",
    "            \"angle_diff\"\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"speed\") * pl.col(\"angle_diff\").cos()).alias(\"s_velocity\"),\n",
    "        (pl.col(\"speed\") * pl.col(\"angle_diff\").sin()).alias(\"d_velocity\"),\n",
    "        ((DIST_2_FRONT_m * pl.col(\"angle_diff\").cos()) + pl.col(\"s\")).alias(\"front_s\"),\n",
    "        (pl.col(\"s\") - (DIST_2_BACK_m * pl.col(\"angle_diff\").cos())).alias(\"back_s\"),\n",
    "        # do the vehicle length\n",
    "        (GLE_m * pl.col(\"angle_diff\").cos()).alias(\"length_s\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "veh_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lable Contigous Segments on the Lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_df = (\n",
    "    veh_df.with_columns(\n",
    "        pl.col(\"lane\").fill_null(\"\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        (\n",
    "            (pl.col(\"lane\").shift(1) != pl.col(\"lane\"))\n",
    "            & (pl.col(\"lane\").shift(1) != \"\")\n",
    "        ).alias(\"sequence\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"sequence\").cum_sum() * (pl.col(\"lane\") != \"\")).alias(\"sequence_id\"),\n",
    "    )\n",
    "    .filter(pl.col(\"sequence_id\") != 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Radar Trajectories \n",
    "\n",
    "Both the prcessed and unprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Radar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import polars as pl\n",
    "from src.radar import CalibratedRadar\n",
    "from src.pipelines.open_file import prep_df\n",
    "from src.pipelines.association import add_front_back_s\n",
    "from src.pipelines.kalman_filter import prepare_frenet_measurement\n",
    "\n",
    "\n",
    "radar_obj = CalibratedRadar(\n",
    "    radar_location_path=ROOT / \"configuration\" / \"march_calibrated.yaml\",\n",
    ")\n",
    "\n",
    "\n",
    "raw_radar_df = (\n",
    "    pl.scan_parquet(\n",
    "        Path(os.environ.get(\"RAW_DATA_DIR\")).joinpath(\"*.parquet\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"epoch_time\").dt.replace_time_zone(\"UTC\"),\n",
    "    )\n",
    "    .with_context(veh_df.lazy())\n",
    "    .filter(\n",
    "        pl.col(\"epoch_time\").is_between(\n",
    "            pl.col(\"gps_time\").min() - timedelta(minutes=1),\n",
    "            pl.col(\"gps_time\").max() + timedelta(minutes=1),\n",
    "        )\n",
    "    )\n",
    "    .collect()\n",
    "    .lazy()\n",
    "    .pipe(prep_df, f=radar_obj)\n",
    "    .pipe(\n",
    "        network.map_to_lane,\n",
    "        dist_upper_bound=8,\n",
    "        utm_x_col=\"utm_x\",\n",
    "        utm_y_col=\"utm_y\",\n",
    "    )\n",
    "    .filter(pl.col(\"name\").is_not_null())\n",
    "    .with_columns(\n",
    "        back_x=(\n",
    "            (pl.col(\"heading_utm\").cos() * pl.col(\"f32_distanceToBack_m\"))\n",
    "            + pl.col(\"utm_x\")\n",
    "        ),\n",
    "        back_y=(\n",
    "            (pl.col(\"heading_utm\").sin() * pl.col(\"f32_distanceToBack_m\"))\n",
    "            + pl.col(\"utm_y\")\n",
    "        ),\n",
    "        front_x=(\n",
    "            (pl.col(\"heading_utm\").cos() * pl.col(\"f32_distanceToFront_m\"))\n",
    "            + pl.col(\"utm_x\")\n",
    "        ),\n",
    "        front_y=(\n",
    "            (pl.col(\"heading_utm\").sin() * pl.col(\"f32_distanceToFront_m\"))\n",
    "            + pl.col(\"utm_y\")\n",
    "        ),\n",
    "    )\n",
    "    .rename(\n",
    "        {\n",
    "            \"name\": \"lane\",\n",
    "            \"angle\": \"heading_lane\",\n",
    "        }\n",
    "    )\n",
    "    .pipe(prepare_frenet_measurement)\n",
    "    .pipe(add_front_back_s, use_median_length=False)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Processed Radar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_radar_df = pl.read_parquet(\n",
    "    ROOT / \"data\" / \"merged_october.parquet\",\n",
    ")\n",
    "\n",
    "processed_radar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Optimal Time Offset b/ Radar and GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_df = veh_df.with_columns(\n",
    "    (pl.col(\"lane\").str.slice(-1, 1).cast(pl.UInt32) - 1).alias(\"lane_index\"),\n",
    "    pl.concat_str(pl.col(\"lane\").str.slice(0, 3), pl.lit(\"1\")).alias(\"lane\"),\n",
    "    pl.col(\"gps_time\").cast(raw_radar_df[\"epoch_time\"].dtype).alias(\"epoch_time\"),\n",
    ")\n",
    "\n",
    "veh_df = veh_df.filter(pl.count().over(\"sequence_id\") > 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Function to Calculate the Offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_error(veh_df, radar_df, offset):\n",
    "    # this function treis to eagerly join the two dataframes\n",
    "    return (\n",
    "        veh_df\n",
    "        # .lazy()\n",
    "        .with_columns(\n",
    "            pl.col(\"epoch_time\") + timedelta(seconds=offset),\n",
    "        )\n",
    "        .filter(pl.col(\"epoch_time\").is_in(radar_df[\"epoch_time\"].unique()))\n",
    "        # .sort(\"s\")\n",
    "        .join(\n",
    "            (\n",
    "                radar_df\n",
    "                # .sort(\"s\").with_columns(pl.col(\"s\").alias(\"s_radar\"))\n",
    "            ),\n",
    "            # on=\"s\",\n",
    "            on=[\"epoch_time\", \"lane\", \"lane_index\"],\n",
    "            suffix=\"_radar\",\n",
    "            # strategy='nearest'\n",
    "            # how=\"inner\",\n",
    "        )\n",
    "        .with_columns(\n",
    "            # calculate the rms error\n",
    "            ((pl.col(\"s\") - pl.col(\"s_radar\")) ** 2).alias(\"s_squared_error\"),\n",
    "            ((pl.col(\"speed\") - pl.col(\"speed_radar\")) ** 2).alias(\n",
    "                \"s_velocity_squared_error\"\n",
    "            ),\n",
    "        )\n",
    "        .group_by([\"vehicle_id\", \"sequence_id\"])\n",
    "        .agg(\n",
    "            (pl.col(\"s_squared_error\").sum() / pl.count()).sqrt().alias(\"s_rmse\"),\n",
    "            (pl.col(\"s_velocity_squared_error\").sum() / pl.count())\n",
    "            .sqrt()\n",
    "            .alias(\"s_velocity_rmse\"),\n",
    "        )\n",
    "        .group_by(\"sequence_id\")\n",
    "        .agg(\n",
    "            pl.col(\"vehicle_id\")\n",
    "            .gather(pl.col(\"s_rmse\").arg_min())\n",
    "            .alias(\"min_rmse_vehicle_id\"),\n",
    "            pl.col(\"vehicle_id\")\n",
    "            .gather(pl.col(\"s_velocity_rmse\").arg_min())\n",
    "            .alias(\"min_s_velocity_rmse_vehicle_id\"),\n",
    "            pl.col(\"s_rmse\").min().alias(\"min_rmse\"),\n",
    "            pl.col(\"s_velocity_rmse\").min().alias(\"min_s_velocity_rmse\"),\n",
    "        )\n",
    "        .explode([\"min_rmse_vehicle_id\", \"min_s_velocity_rmse_vehicle_id\"])\n",
    "        # .collect()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Optimal Offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_summary = []\n",
    "\n",
    "for second_offset in range(-200, 0, 1):\n",
    "    error_df = offset_error(\n",
    "        veh_df.with_columns(\n",
    "            pl.col(\"gps_time\")\n",
    "            .cast(raw_radar_df[\"epoch_time\"].dtype)\n",
    "            .alias(\"epoch_time\"),\n",
    "            pl.col(\"front_s\").cast(float).alias(\"s\"),\n",
    "        ),\n",
    "        processed_radar_df.with_columns(\n",
    "            pl.col(\"front_s_smooth\").cast(float).alias(\"s\"),\n",
    "            pl.col(\"s_velocity_smooth\").alias(\"speed_radar\"),\n",
    "            pl.col(\"lane_index\").cast(pl.UInt32).alias(\"lane_index\"),\n",
    "            # pl.col('object_id').alias('vehicle_id'),\n",
    "        ),\n",
    "        second_offset / 10,\n",
    "    )\n",
    "\n",
    "    error_summary.append(\n",
    "        error_df.with_columns(\n",
    "            pl.lit(second_offset / 10).alias(\"second_offset\"),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "error_summary_df = pl.concat(error_summary)\n",
    "\n",
    "error_summary_df_seconds = error_summary_df.group_by(\"second_offset\").agg(\n",
    "    pl.col(\"min_rmse\").mean().alias(\"min_rmse\"),\n",
    "    pl.col(\"min_s_velocity_rmse\").mean().alias(\"min_s_velocity_rmse\"),\n",
    ")\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=error_summary_df_seconds[\"second_offset\"],\n",
    "        y=error_summary_df_seconds[\"min_rmse\"],\n",
    "        name=\"RMSE\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=error_summary_df_seconds[\"second_offset\"],\n",
    "        y=error_summary_df_seconds[\"min_s_velocity_rmse\"],\n",
    "        name=\"Speed RMSE\",\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    xaxis_title=\"Second Offset\",\n",
    "    yaxis_title=\"RMSE\",\n",
    "    yaxis2_title=\"Speed RMSE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Offset and Try to Cluster the Matching Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_offset = error_summary_df_seconds.sort(\"min_rmse\")[\"second_offset\"][0]\n",
    "\n",
    "print(f\"Optimal offset: {optimal_offset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Vehicles that Best Match the GPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_error(\n",
    "    veh_df.with_columns(\n",
    "        pl.col(\"gps_time\").cast(raw_radar_df[\"epoch_time\"].dtype).alias(\"epoch_time\"),\n",
    "        pl.col(\"front_s\").cast(float).alias(\"s\"),\n",
    "    ),\n",
    "    processed_radar_df.with_columns(\n",
    "        pl.col(\"front_s_smooth\").cast(float).alias(\"s\"),\n",
    "        pl.col(\"s_velocity_smooth\").alias(\"speed_radar\"),\n",
    "        pl.col(\"lane_index\").cast(pl.UInt32).alias(\"lane_index\"),\n",
    "        # pl.col('object_id').alias('vehicle_id'),\n",
    "    ),\n",
    "    optimal_offset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_df = (\n",
    "    veh_df.with_columns(\n",
    "        (pl.col(\"gps_time\") + timedelta(seconds=optimal_offset))\n",
    "        .cast(processed_radar_df[\"epoch_time\"].dtype)\n",
    "        .alias(\"epoch_time\"),\n",
    "        pl.col(\"front_s\").cast(float).alias(\"s\"),\n",
    "    )\n",
    "    .filter(pl.col(\"epoch_time\").is_in(processed_radar_df[\"epoch_time\"].unique()))\n",
    "    .join(\n",
    "        processed_radar_df.select(\n",
    "            [\n",
    "                pl.col(\"front_s_smooth\").cast(float).alias(\"s_radar\"),\n",
    "                pl.col(\"s_velocity_smooth\").alias(\"speed_radar\"),\n",
    "                pl.col(\"lane_index\").cast(pl.UInt32).alias(\"lane_index\"),\n",
    "                \"epoch_time\",\n",
    "                \"vehicle_id\",\n",
    "                \"lane\",\n",
    "            ]\n",
    "        ),\n",
    "        on=[\"epoch_time\", \"lane\", \"lane_index\"],\n",
    "        # how=\"outer\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        # calculate the rms error\n",
    "        ((pl.col(\"s\") - pl.col(\"s_radar\")) ** 2).alias(\"s_squared_error\"),\n",
    "        ((pl.col(\"speed\") - pl.col(\"speed_radar\")) ** 2).alias(\n",
    "            \"s_velocity_squared_error\"\n",
    "        ),\n",
    "    )\n",
    "    .group_by([\"vehicle_id\", \"sequence_id\"])\n",
    "    .agg(\n",
    "        (pl.col(\"s_squared_error\").sum() / pl.count()).sqrt().alias(\"s_rmse\"),\n",
    "        (pl.col(\"s_velocity_squared_error\").sum() / pl.count())\n",
    "        .sqrt()\n",
    "        .alias(\"s_velocity_rmse\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to make two clusters\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=2).fit(scatter_df[[\"s_rmse\", \"s_velocity_rmse\"]].to_numpy())\n",
    "\n",
    "scatter_df = scatter_df.with_columns(\n",
    "    cluster=pl.Series(dbscan.labels_).cast(str),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    scatter_df.to_pandas(),\n",
    "    x=\"s_rmse\",\n",
    "    y=\"s_velocity_rmse\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"vehicle_id\", \"sequence_id\"],\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_cluster = (\n",
    "    scatter_df.group_by(\"cluster\")\n",
    "    .agg(\n",
    "        pl.col(\"s_rmse\").mean().alias(\"s_rmse\"),\n",
    "    )\n",
    "    .sort(\"s_rmse\")[\"cluster\"][0]\n",
    ")\n",
    "\n",
    "print(f\"Good cluster: {good_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_matches = scatter_df.filter(pl.col(\"cluster\") == good_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ids = (\n",
    "    processed_radar_df.filter(\n",
    "        pl.col(\"vehicle_id\").is_in(good_matches[\"vehicle_id\"].unique())\n",
    "    )[[\"vehicle_id\", \"object_id\"]]\n",
    "    .unique()\n",
    "    .explode(\"object_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Positional Error\n",
    "\n",
    "We filter out short matches, as this happened when the vehicle crossed the lanes during turning movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_df = veh_df.filter(pl.count().over(\"sequence_id\") > 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_error_df = raw_radar_df.filter(\n",
    "    pl.col(\"object_id\").is_in(object_ids[\"object_id\"])\n",
    ").join(\n",
    "    object_ids,\n",
    "    on=\"object_id\",\n",
    ")\n",
    "\n",
    "# raw_error_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Paired DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_radar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df = (\n",
    "    veh_df.with_columns(\n",
    "        (\n",
    "            pl.col(\"gps_time\").cast(raw_error_df[\"epoch_time\"].dtype)\n",
    "            + timedelta(seconds=optimal_offset)\n",
    "        ).alias(\"epoch_time\"),\n",
    "        pl.col(\"lane_index\").cast(int),\n",
    "    )\n",
    "    .join(\n",
    "        processed_radar_df.select(\n",
    "            [\n",
    "                \"epoch_time\",\n",
    "                \"front_s_smooth\",\n",
    "                \"back_s_smooth\",\n",
    "                \"s_velocity_smooth\",\n",
    "                \"d_smooth\",\n",
    "                \"d_velocity_smooth\",\n",
    "                \"vehicle_id\",\n",
    "                \"lane\",\n",
    "                \"front_x\",\n",
    "                \"front_y\",\n",
    "                \"back_x\",\n",
    "                \"back_y\",\n",
    "                pl.col(\"lane_index\").cast(int),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            ((pl.col(\"s_velocity_smooth\") ** 2) + (pl.col(\"d_velocity_smooth\") ** 2))\n",
    "            .sqrt()\n",
    "            .alias(\"speed_smooth\"),\n",
    "        )\n",
    "        .filter(pl.col(\"vehicle_id\").is_in(good_matches[\"vehicle_id\"].unique())),\n",
    "        on=[\"epoch_time\", \"lane\", \"lane_index\"],\n",
    "        suffix=\"_rts\",\n",
    "    )\n",
    "    .join(\n",
    "        raw_error_df.select(\n",
    "            [\n",
    "                \"epoch_time\",\n",
    "                \"front_s\",\n",
    "                \"back_s\",\n",
    "                \"s_velocity\",\n",
    "                \"d\",\n",
    "                \"d_velocity\",\n",
    "                pl.col(\"f32_velocityInDir_mps\").alias(\"speed_raw\"),\n",
    "                pl.col(\"utm_x\").alias(\"x_raw\"),\n",
    "                pl.col(\"utm_y\").alias(\"y_raw\"),\n",
    "                \"vehicle_id\",\n",
    "                \"object_id\",\n",
    "                \"ip\",\n",
    "            ]\n",
    "        ),\n",
    "        on=[\"epoch_time\", \"vehicle_id\"],\n",
    "        suffix=\"_raw\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = paired_df.filter(pl.col(\"front_s_smooth\").is_not_null())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "error_df = error_df.sort(\"epoch_time\")\n",
    "\n",
    "# plot all the EB trajectories\n",
    "for g, seq_df in error_df.group_by(\"sequence_id\"):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=seq_df[\"epoch_time\"],\n",
    "            y=seq_df[\"front_s\"],\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"Sequence {g}\",\n",
    "            marker_color=\"black\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=seq_df[\"epoch_time\"],\n",
    "            y=seq_df[\"front_s_smooth\"],\n",
    "            mode=\"markers+lines\",\n",
    "            name=f\"Sequence {g}\",\n",
    "            marker_color=\"red\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # plot the trajectories\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=seq_df[\"epoch_time\"],\n",
    "            y=seq_df[\"front_s_raw\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"Sequence {g}\",\n",
    "            marker_color=\"green\",\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# if SHOW_FIGS:\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def element_error(\n",
    "    df: pl.DataFrame, true_col: str, pred_col: str, out_col: str = None, pearsonr_col: str = None, mae_col: str = None\n",
    ") -> pl.DataFrame:\n",
    "    out_col = out_col or f\"{true_col}_se\"\n",
    "    pearsonr_col = pearsonr_col or f\"{true_col}_pearsonr\"\n",
    "    mae_col = mae_col or f\"{true_col}_mae\"\n",
    "    # this some f sh!t\n",
    "    def lit_pearson_r(_df):\n",
    "        return _df.with_columns(\n",
    "            pl.lit(\n",
    "                pearsonr(\n",
    "                    _df[true_col].to_numpy(),\n",
    "                    _df[pred_col].to_numpy(),\n",
    "                )[0]\n",
    "            ).alias(pearsonr_col)\n",
    "        )\n",
    "\n",
    "    return df.with_columns(\n",
    "        ((pl.col(true_col) - pl.col(pred_col)) ** 2).alias(out_col),\n",
    "        ((pl.col(true_col) - pl.col(pred_col)).abs()).alias(mae_col),\n",
    "    ).pipe(lit_pearson_r)\n",
    "\n",
    "\n",
    "# def mse(df: pl.DataFrame, true_col: str, pred_col: str) -> float:\n",
    "#     return se(df, true_col, pred_col, \"se\")[\"se\"].mean()\n",
    "\n",
    "\n",
    "# def rmse(df: pl.DataFrame, true_col: str, pred_col: str) -> float:\n",
    "#     return mse(df, true_col, pred_col) ** (1 / 2)\n",
    "\n",
    "\n",
    "# def pearsonr_(df: pl.DataFrame, true_col: str, pred_col: str) -> float:\n",
    "#     # not waht this is really supposed to show, as obvoiusly the two are correlated\n",
    "#     return pearsonr(df[true_col].to_numpy(), df[pred_col].to_numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_error_df = error_df.filter(\n",
    "    pl.col(\"vehicle_id\").cum_count().over([\"epoch_time\", \"vehicle_id\"]) < 1\n",
    ")\n",
    "\n",
    "smoothed_error_df = (\n",
    "    smoothed_error_df.pipe(\n",
    "        element_error,\n",
    "        true_col=\"front_s\",\n",
    "        pred_col=\"front_s_smooth\",\n",
    "    )\n",
    "    .pipe(\n",
    "        element_error,\n",
    "        true_col=\"s_velocity\",\n",
    "        pred_col=\"s_velocity_smooth\",\n",
    "    )\n",
    "    .pipe(\n",
    "        element_error,\n",
    "        true_col=\"back_s\",\n",
    "        pred_col=\"back_s_smooth\",\n",
    "    )\n",
    "    .pipe(\n",
    "        element_error,\n",
    "        true_col=\"speed\",\n",
    "        pred_col=\"speed_smooth\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        (\n",
    "            (pl.col(f\"{loc}_x\") - pl.col(f\"{loc}_x_rts\")).pow(2)\n",
    "            + (pl.col(f\"{loc}_y\") - pl.col(f\"{loc}_y_rts\")).pow(2)\n",
    "        ).alias(f\"{loc}_xy_se\")\n",
    "        for loc in [\"front\", \"back\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def mse_pl(col: pl.col) -> pl.Expr:\n",
    "    return col.mean()\n",
    "\n",
    "\n",
    "def rmse_pl(col: pl.col) -> pl.Expr:\n",
    "    return mse_pl(col) ** (1 / 2)\n",
    "\n",
    "\n",
    "smoothed_error_df.group_by(\"sequence_id\").agg(\n",
    "    pl.col(\"^.*_se$\").mean().name.map(lambda x: f\"{x.replace('_se', '')}_mse\"),\n",
    "    pl.col(\"^.*_se$\").mean().sqrt().name.map(lambda x: f\"{x.replace('_se', '')}_rmse\"),\n",
    "    pl.col(\"^.*_pearsonr$\").first(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_error_df = error_df.filter(\n",
    "    pl.col(\"vehicle_id\").cum_count().over([\"epoch_time\", \"vehicle_id\"]) < 1\n",
    ")\n",
    "\n",
    "smoothed_error_df = (\n",
    "    smoothed_error_df.pipe(\n",
    "        element_error,\n",
    "        true_col=\"front_s\",\n",
    "        pred_col=\"front_s_smooth\",\n",
    "    )\n",
    "    .pipe(\n",
    "        element_error,\n",
    "        true_col=\"s_velocity\",\n",
    "        pred_col=\"s_velocity_smooth\",\n",
    "    )\n",
    "    .pipe(\n",
    "        element_error,\n",
    "        true_col=\"back_s\",\n",
    "        pred_col=\"back_s_smooth\",\n",
    "    )\n",
    "    .pipe(\n",
    "        element_error,\n",
    "        true_col=\"speed\",\n",
    "        pred_col=\"speed_smooth\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        (\n",
    "            (pl.col(f\"{loc}_x\") - pl.col(f\"{loc}_x_rts\")).pow(2)\n",
    "            + (pl.col(f\"{loc}_y\") - pl.col(f\"{loc}_y_rts\")).pow(2)\n",
    "        ).alias(f\"{loc}_xy_se\")\n",
    "        for loc in [\"front\", \"back\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def mse_pl(col: pl.col) -> pl.Expr:\n",
    "    return col.mean()\n",
    "\n",
    "\n",
    "def rmse_pl(col: pl.col) -> pl.Expr:\n",
    "    return mse_pl(col) ** (1 / 2)\n",
    "\n",
    "\n",
    "smoothed_error_df.group_by(\"sequence_id\").agg(\n",
    "    pl.col(\"^.*_se$\").mean().name.map(lambda x: f\"{x.replace('_se', '')}_mse\"),\n",
    "    pl.col(\"^.*_se$\").mean().sqrt().name.map(lambda x: f\"{x.replace('_se', '')}_rmse\"),\n",
    "    pl.col(\"^.*_pearsonr$\").first(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_error_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
