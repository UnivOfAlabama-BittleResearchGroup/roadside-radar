{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associating and Joining Trajectories\n",
    "\n",
    "This relies on the output of [./vectorized_filter.ipynb](./vectorized_filter.ipynb) -> [./lane_classification.ipynb](./lane_classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# find the root of the project\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "while not ROOT.joinpath(\".git\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "# add the root to the python path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import polars as pl\n",
    "from pomegranate.distributions import Normal\n",
    "from pomegranate.gmm import GeneralMixtureModel\n",
    "\n",
    "\n",
    "# load the environment variables\n",
    "dotenv.load_dotenv(ROOT.joinpath(\".env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = pl.scan_parquet(\n",
    "    ROOT.joinpath(\"notebooks/clean_workflow/data/imm_filtered_lanes.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pl.scan_parquet(\n",
    "    ROOT.joinpath(\"tmp/all_working_processed_1Lane.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in the IP Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "radar_df = radar_df.join(\n",
    "    raw_df.select(\n",
    "        list(set(raw_df.columns).difference(set(radar_df.columns)))\n",
    "        + [\"object_id\", \"epoch_time\"]\n",
    "    ),\n",
    "    on=[\"object_id\", \"epoch_time\"],\n",
    "    how=\"inner\",\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Leader-Follower Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Whether a Vehicle is Heading Towards or Away from the Radar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = (\n",
    "    radar_df.with_columns(\n",
    "        (pl.col(\"f32_positionX_m\") ** 2 + pl.col(\"f32_positionY_m\") ** 2)\n",
    "        .sqrt()\n",
    "        .alias(\"distance\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        (\n",
    "            (pl.col(\"distance\").diff() <= 0)\n",
    "            .backward_fill()\n",
    "            .over(\"object_id\")\n",
    "            .alias(\"towards_radar\")\n",
    "        )\n",
    "    )\n",
    "    # correct the length estimates\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"f32_distanceToFront_m\") * pl.col(\"s_angle_diff\").cos()).alias(\n",
    "                \"distanceToFront_s\"\n",
    "            ),\n",
    "            (pl.col(\"f32_distanceToBack_m\") * pl.col(\"s_angle_diff\").cos()).alias(\n",
    "                \"distanceToBack_s\"\n",
    "            ),\n",
    "            # do the vehicle length\n",
    "            (pl.col(\"f32_length_m\") * pl.col(\"s_angle_diff\").cos()).alias(\"length_s\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        # use the median vehicle length\n",
    "        pl.col(\"length_s\")\n",
    "        .median()\n",
    "        .over(\"object_id\")\n",
    "        .alias(\"median_length_s\")\n",
    "    )\n",
    "    # Make the assumption that the radar picks up the plane of the vehicle closest to the radar\n",
    "    # try to correct for this and get the true centroid of the vehicle\n",
    "    .with_columns(pl.col(\"s_filt\").alias(\"s_centroid\"))\n",
    "    # correct to find the true front and back of the vehicle\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"s_centroid\") + (pl.col(\"median_length_s\") / 2)).alias(\n",
    "                \"backBumper_s\"\n",
    "            ),\n",
    "            (pl.col(\"s_centroid\") - (pl.col(\"median_length_s\") / 2)).alias(\n",
    "                \"frontBumper_s\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Unique Column for Lane - Lane Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = radar_df.with_columns(\n",
    "    pl.struct([\"lane\", \"lane_index\"]).hash().alias(\"lane_hash\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Leader Follower Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.association.pipelines import build_match_df\n",
    "\n",
    "matching_df = build_match_df(\n",
    "    radar_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filters.fusion import mahalanobis_distance, loglikelihood\n",
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "matching_df = (\n",
    "    matching_df.pipe(\n",
    "        mahalanobis_distance,\n",
    "        cutoff=chi2.ppf(0.90, 4),\n",
    "        gpu=True,\n",
    "    )\n",
    "    # p(a = b) = 1 - p(a <> b) = 1 - (p(birth) + p(error) + p())\n",
    "    # If I use a validation gate, then I have to normalize by the area of the gate\n",
    "    # The potenital of using a complicated birth model here is\n",
    "    # for now just rely on the gate\n",
    "    .pipe(loglikelihood, gpu=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Headways and Find the Middle of Leader-Follower Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.association.pipelines import calculate_match_indexes, pipe_gate_headway_calc\n",
    "\n",
    "matching_df = matching_df.filter(\n",
    "    (pl.col(\"s_velocity_filt\").abs() > 5) & (pl.col(\"s_velocity_filt_leader\").abs() > 5)\n",
    ").pipe(\n",
    "    calculate_match_indexes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_matches = (\n",
    "    matching_df.pipe(pipe_gate_headway_calc)\n",
    "    .filter(\n",
    "        (pl.col(\"inside_gate\") > 0.5)\n",
    "        | ((pl.col(\"headway\") < 0.5) & (pl.col(\"headway_std\") < 0.1))\n",
    "    )\n",
    "    .sort(\"epoch_time\")\n",
    "    .unnest(\"pair\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"object_id\").cumcount() + 1).over(\"object_id\").alias(\"following_count\"),\n",
    "        (pl.col(\"object_id\").cumcount() + 1).over(\"leader\").alias(\"leader_count\"),\n",
    "    )\n",
    "    .sort(\"object_id\")\n",
    "    .filter(\n",
    "        (\n",
    "            ~pl.col(\"prediction\")\n",
    "            | ((pl.col(\"following_count\") < 3) & (pl.col(\"leader_count\") < 3))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Graph of Connected Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.association.pipelines import create_vehicle_ids\n",
    "\n",
    "joined_df = radar_df.pipe(\n",
    "    create_vehicle_ids,\n",
    "    match_df=valid_matches,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    joined_df.group_by([\"object_id\"]).agg(\n",
    "        pl.col(\"vehicle_id\").n_unique().alias(\"vehicle_count\")\n",
    "    )[\"vehicle_count\"]\n",
    "    == 1\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark Vehicle Group Ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting.time_space import plot_time_space\n",
    "from datetime import timedelta\n",
    "from src.radar import Filtering\n",
    "\n",
    "\n",
    "# get a 10 minute window\n",
    "plot_df = joined_df.filter(\n",
    "    pl.col(\"epoch_time\").is_between(\n",
    "        joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=40),\n",
    "        joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=45),\n",
    "    )\n",
    "    & (pl.col(\"lane\").str.contains(\"EBL1\"))\n",
    "    & (pl.col(\"lane_index\") == 1)\n",
    "    # (pl.col(\"vehicle_id\") == pl.lit(15420721423209556182))\n",
    "    # pl.col('object_id').is_in([254, 147,])\n",
    ").pipe(Filtering.add_cst_timezone)\n",
    "\n",
    "fig = plot_time_space(\n",
    "    plot_df,\n",
    "    hoverdata=\"object_id\",\n",
    "    vehicle_col=\"vehicle_id\",\n",
    "    s_col=\"s_centroid\",\n",
    "    markers=True,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Positional Error as A Function of Radar Pair & IP Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploting the fact that we are looking at EB lanes and ips for from 136 -> 147 in order\n",
    "e_df = (\n",
    "    joined_df.filter((pl.count().over([\"epoch_time\", \"vehicle_id\"]) > 1))\n",
    "    .sort([\"epoch_time\", \"ip\"], descending=[False, False])\n",
    "    .group_by([\"epoch_time\", \"vehicle_id\"])\n",
    "    .agg(\n",
    "        (pl.col(\"s_centroid\").first() - pl.col(\"s_centroid\")).alias(\"s_error\"),\n",
    "        pl.col(\"ip\").first().alias(\"first_ip\"),\n",
    "        pl.col(\"ip\"),\n",
    "        pl.col(\"object_id\"),\n",
    "        pl.col(\"object_id\").first().alias(\"first_object_id\"),\n",
    "        (pl.col(\"s_centroid\").first() // 5).alias(\"s_pos_binned\"),\n",
    "        pl.col(\"lane\").first().alias(\"lane\"),\n",
    "    )\n",
    "    .explode([\"ip\", \"s_error\", \"object_id\"])\n",
    "    .filter(\n",
    "        (pl.col(\"ip\") != pl.col(\"first_ip\"))\n",
    "        & (\n",
    "            pl.col(\"s_error\").is_between(\n",
    "                pl.col(\"s_error\").quantile(0.05), pl.col(\"s_error\").quantile(0.95)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the error vs. s\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# # make subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=4,\n",
    "#     cols=1,\n",
    "#     shared_xaxes=False,\n",
    "#     vertical_spacing=0.05,\n",
    "#     specs=[[{\"type\": \"scatter\"}], [{\"type\": \"scatter\"}], [{\"type\": \"scatter\"}], [{\"type\": \"scatter\"}]],\n",
    "# )\n",
    "\n",
    "# for i, (r1, r2) in enumerate([(\"137\", \"141\"), (\"136\", \"137\"), (\"141\", \"142\"), (\"142\", \"146\")]):\n",
    "#     plot_df = (\n",
    "#         e_df.filter(pl.col(\"first_ip\").str.contains(r1) & pl.col(\"ip\").str.contains(r2))\n",
    "#         .group_by(\"s_pos_binned\")\n",
    "#         .agg(\n",
    "#             pl.col(\"s_error\").mean().alias(\"s_error_avg\"),\n",
    "#             pl.col(\"s_error\").quantile(0.95).alias(\"s_error_q95\"),\n",
    "#             pl.col(\"s_error\").quantile(0.05).alias(\"s_error_q05\"),\n",
    "#             pl.col(\"s_error\").quantile(0.5).alias(\"s_error_q50\"),\n",
    "#             pl.col(\"s_error\").count().alias(\"count\"),\n",
    "#         )\n",
    "#         .sort(\"s_pos_binned\", descending=False)\n",
    "#         .with_columns(pl.col(\"s_pos_binned\") * 5.0)\n",
    "#     )\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=plot_df[\"s_pos_binned\"],\n",
    "#             y=plot_df[\"s_error_q50\"],\n",
    "#             name=f\"{r1} -> {r2}\",\n",
    "#         ),\n",
    "#         row=1 + i,\n",
    "#         col=1,\n",
    "#     )\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=plot_df[\"s_pos_binned\"],\n",
    "#             y=plot_df[\"s_error_q95\"],\n",
    "#             # make it a filled area\n",
    "#             line=dict(\n",
    "#                 color=\"rgba(255,255,255,0)\",\n",
    "#             ),\n",
    "#             showlegend=False,\n",
    "#         ),\n",
    "#         row=1 + i,\n",
    "#         col=1,\n",
    "#     )\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=plot_df[\"s_pos_binned\"],\n",
    "#             y=plot_df[\"s_error_q05\"],\n",
    "#             fill=\"tonexty\",\n",
    "#             showlegend=False,\n",
    "#         ),\n",
    "#         row=1 + i,\n",
    "#         col=1,\n",
    "#     )\n",
    "\n",
    "# # make the x-axis descending\n",
    "# fig.update_xaxes(autorange=\"reversed\")\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Correction DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_pairs = dict(\n",
    "    [\n",
    "        [136, 137],\n",
    "        [137, 141],\n",
    "        [141, 142],\n",
    "        [142, 146],\n",
    "        [146, 147],\n",
    "    ]\n",
    ")\n",
    "\n",
    "correction_df = (\n",
    "    e_df.group_by(\n",
    "        [\n",
    "            \"first_ip\",\n",
    "            \"ip\",\n",
    "            \"lane\",\n",
    "        ]\n",
    "    )\n",
    "    .agg(\n",
    "        pl.col(\"s_error\").mean().alias(\"mean_s_error\"),\n",
    "        pl.col(\"s_error\").median().alias(\"median_s_error\"),\n",
    "        pl.col(\"s_error\").std().alias(\"std_s_error\"),\n",
    "        pl.col(\"s_error\").count().alias(\"count\"),\n",
    "    )\n",
    "    .sort([\"first_ip\", \"ip\"])\n",
    "    .with_columns(\n",
    "        pl.col([\"first_ip\", \"ip\"])\n",
    "        .map_batches(lambda x: x.str.slice(-3).cast(int))\n",
    "        .map_alias(lambda x: f\"{x}_int\")\n",
    "    )\n",
    "    .filter(pl.col(\"ip_int\") == pl.col(\"first_ip_int\").map_dict(keep_pairs))\n",
    "    .sort([\"first_ip\", \"ip\"])\n",
    "    .with_columns(pl.col(\"median_s_error\").cumsum().over(\"lane\").alias(\"correction\"))\n",
    ")\n",
    "\n",
    "\n",
    "correction_df.sort('lane').head(10)\n",
    "# keep_pairs = [[136, 147], [147, 136], [136, 254], [254, 136], [147, 254], [254, 147]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.join(\n",
    "    correction_df.select([\"ip\", \"lane\", \"correction\"]), \n",
    "    on=[\"ip\", \"lane\"],\n",
    "    how='left',\n",
    ").with_columns((pl.col(\"s_centroid\") + pl.col(\"correction\").fill_null(0)).alias(\"corrected_s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting.time_space import plot_time_space\n",
    "from datetime import timedelta\n",
    "from src.radar import Filtering\n",
    "\n",
    "\n",
    "# get a 10 minute window\n",
    "plot_df = joined_df.filter(\n",
    "    pl.col(\"epoch_time\").is_between(\n",
    "        joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=40),\n",
    "        joined_df[\"epoch_time\"].min() + timedelta(hours=0, minutes=45),\n",
    "    )\n",
    "    & (pl.col(\"lane\").str.contains(\"WBL1\"))\n",
    "    & (pl.col(\"lane_index\") == 1)\n",
    "    # (pl.col(\"vehicle_id\") == pl.lit(15420721423209556182))\n",
    "    # pl.col('object_id').is_in([254, 147,])\n",
    ").pipe(Filtering.add_cst_timezone)\n",
    "\n",
    "fig = plot_time_space(\n",
    "    plot_df,\n",
    "    hoverdata=\"object_id\",\n",
    "    vehicle_col=\"vehicle_id\",\n",
    "    s_col=\"corrected_s\",\n",
    "    markers=True,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_df.write_parquet(\n",
    "    ROOT / 'notebooks' / \"clean_workflow\" / \"data\" / \"offsets.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
