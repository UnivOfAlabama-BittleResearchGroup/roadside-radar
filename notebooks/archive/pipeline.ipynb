{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# find the root of the project\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import polars as pl\n",
    "import dotenv\n",
    "\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "while not ROOT.joinpath(\".git\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "# add the root to the python path\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "\n",
    "dotenv.load_dotenv(ROOT.joinpath(\".env\"))\n",
    "\n",
    "from src.utils import check_gpu_available\n",
    "\n",
    "GPU = check_gpu_available()\n",
    "print(f\"GPU available: {GPU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from src.geometry import RoadNetwork\n",
    "\n",
    "\n",
    "mainline_net = RoadNetwork(\n",
    "    lane_gdf=gpd.read_file(ROOT / \"data/mainline_lanes.geojson\"),\n",
    "    keep_lanes=[\"EBL1\", \"WBL1\"],\n",
    "    step_size=0.01,\n",
    ")\n",
    "\n",
    "full_net = RoadNetwork(\n",
    "    lane_gdf=gpd.read_file(ROOT / \"data/mainline_lanes.geojson\"),\n",
    "    keep_lanes=None,\n",
    "    step_size=0.01,\n",
    ")\n",
    "\n",
    "LANE_WIDTH = 3.55\n",
    "LANE_NUM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/HDD/max/radar-data\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"RAW_DATA_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_df = (\n",
    "    pl.read_csv(\n",
    "        ROOT / \"data\" / \"vehicle_drives\" / \"2023-10-31.csv\",\n",
    "    )\n",
    "    .drop(\"\")\n",
    "    .with_columns(\n",
    "        pl.col(\"gps_time\").str.strptime(\n",
    "            dtype=pl.Datetime(\n",
    "                time_unit=\"us\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_df.write_parquet(\n",
    "    \"/home/max/Desktop/DOECV2X/Working_Papers/radar-trajectory-extraction/validation_dataset/2023-10-31.Probe.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2023-10-31 16:43:11.400000+00:00', '2023-10-31 17:17:47.300000+00:00')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(veh_df[\"gps_time\"].min()), str(veh_df[\"gps_time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "radar_df = (\n",
    "    pl.scan_parquet(\n",
    "        Path(os.environ.get(\"RAW_DATA_DIR\")).joinpath(\"*.parquet\"),\n",
    "        # Path(os.environ.get(\"RAW_DATA_DIR\")).joinpath(\"1698774489178.parquet\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"epoch_time\").dt.replace_time_zone(\"UTC\"),\n",
    "    )\n",
    "    .with_context(veh_df.lazy())\n",
    "    .filter(\n",
    "        pl.col(\"epoch_time\").is_between(\n",
    "            pl.col(\"gps_time\").min() - timedelta(minutes=2),\n",
    "            pl.col(\"gps_time\").max() + timedelta(minutes=2),\n",
    "        )\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df.write_parquet(\n",
    "    \"/home/max/Desktop/DOECV2X/Working_Papers/radar-trajectory-extraction/raw_data/2023-10-31.Radar.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import polars as pl\n",
    "from src.radar import CalibratedRadar\n",
    "from src.pipelines.open_file import prep_df\n",
    "\n",
    "\n",
    "USE_FRONT = False\n",
    "\n",
    "\n",
    "radar_obj = CalibratedRadar(\n",
    "    radar_location_path=ROOT / \"configuration\" / \"october_calibrated.yaml\",\n",
    ")\n",
    "\n",
    "\n",
    "radar_df = (\n",
    "    pl.scan_parquet(\n",
    "        Path(os.environ.get(\"RAW_DATA_DIR\")).joinpath(\"*.parquet\"),\n",
    "        # Path(os.environ.get(\"RAW_DATA_DIR\")).joinpath(\"1698774489178.parquet\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"epoch_time\").dt.replace_time_zone(\"UTC\"),\n",
    "    )\n",
    "    .with_context(veh_df.lazy())\n",
    "    .filter(\n",
    "        pl.col(\"epoch_time\").is_between(\n",
    "            pl.col(\"gps_time\").min() - timedelta(minutes=2),\n",
    "            pl.col(\"gps_time\").max() + timedelta(minutes=2),\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"f32_positionX_m\") ** 2 + pl.col(\"f32_positionY_m\") ** 2)\n",
    "        .sqrt()\n",
    "        .alias(\"dist\"),\n",
    "    )\n",
    "    .filter(\n",
    "        pl.when(pl.col(\"ip\").is_in([\"10.160.7.141\", \"10.160.7.137\"]))\n",
    "        .then(pl.col(\"dist\") < 300)\n",
    "        .otherwise(pl.lit(True))\n",
    "    )\n",
    "    .collect()\n",
    "    .lazy()\n",
    "    .pipe(prep_df, f=radar_obj)\n",
    "    .collect()\n",
    "    .pipe(\n",
    "        mainline_net.map_to_lane,\n",
    "        dist_upper_bound=LANE_WIDTH * LANE_NUM\n",
    "        - (LANE_WIDTH / 2)\n",
    "        + 0.5,  # centered on one of the lanes,\n",
    "        utm_x_col=\"utm_x\",\n",
    "        utm_y_col=\"utm_y\",\n",
    "    )\n",
    "    .filter(pl.col(\"name\").is_not_null())\n",
    "    .rename(\n",
    "        {\n",
    "            \"name\": \"lane\",\n",
    "            \"angle\": \"heading_lane\",\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Shenglin's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_df = (\n",
    "#     radar_df.filter(pl.col(\"ip\").str.contains(\"142\") | pl.col(\"ip\").str.contains(\"141\"))\n",
    "#     .select(\n",
    "#         [\n",
    "#             \"epoch_time_cst\",\n",
    "#             \"object_id\",\n",
    "#             \"utm_x\",\n",
    "#             \"utm_y\",\n",
    "#             pl.col(\"f32_velocityInDir_mps\").alias(\"speed\"),\n",
    "#             \"ip\",\n",
    "#             \"f32_length_m\",\n",
    "#             \"f32_distanceToFront_m\",\n",
    "#             \"f32_distanceToBack_m\",\n",
    "#             \"heading\",\n",
    "#         ]\n",
    "#     )\n",
    "#     .filter(\n",
    "#         pl.col(\"epoch_time_cst\").is_between(\n",
    "#             pl.lit(\"2023-10-31 11:58:00\").str.strptime(\n",
    "#                 pl.Datetime(time_unit=\"ns\", time_zone=\"US/Central\"),\n",
    "#             ),\n",
    "#             pl.lit(\"2023-10-31 13:02:00\").str.strptime(\n",
    "#                 pl.Datetime(time_unit=\"ns\", time_zone=\"US/Central\"),\n",
    "#             ),\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# save_df.write_csv('/home/max/Development/roadside-radar/data/10_31_2023_12_13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.kalman_filter import (\n",
    "    prepare_frenet_measurement,\n",
    "    build_extension,\n",
    "    add_timedelta,\n",
    "    build_kalman_id,\n",
    "    filter_short_trajectories,\n",
    ")\n",
    "\n",
    "\n",
    "prediction_length = 4\n",
    "\n",
    "print(radar_df.shape)\n",
    "\n",
    "radar_df = (\n",
    "    radar_df.pipe(add_timedelta, vehicle_id_col=[\"object_id\", \"lane\"])\n",
    "    .pipe(build_kalman_id, split_time_delta=prediction_length + 0.1)\n",
    "    .pipe(filter_short_trajectories, minimum_distance_m=5, minimum_duration_s=2)\n",
    "    .pipe(prepare_frenet_measurement)\n",
    "    .pipe(build_extension, seconds=prediction_length)\n",
    "    .pipe(add_timedelta)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(radar_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df.select(pl.col(\"distanceToFront_s\").std().over(\"object_id\")).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df.select(pl.col(\"distanceToBack_s\").std().over(\"object_id\")).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df['epoch_time'].dt.min(), test_df['epoch_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# from src.plotting.time_space import plot_time_space\n",
    "# from datetime import timedelta\n",
    "\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # get a 10 minute window\n",
    "# plot_df = (\n",
    "#     radar_df\n",
    "#     .with_columns((pl.col(\"s\") + pl.col('distanceToFront_s')).alias(\"s\"))\n",
    "#     .pipe(radar_obj.add_cst_timezone)\n",
    "#     .filter(\n",
    "#     #     pl.col(\"epoch_time_cst\").is_between(\n",
    "#     #         pl.lit(\"2023-03-13 06:50:00\").str.strptime(pl.Datetime(time_unit=\"ns\", time_zone='US/Central'), ),\n",
    "#     #         pl.lit(\"2023-03-13 06:59:00\").str.strptime(pl.Datetime(time_unit=\"ns\", time_zone='US/Central'), ),\n",
    "#     #     )\n",
    "#         (pl.col(\"lane\").str.contains(\"E\"))\n",
    "#         # & (pl.col(\"lane_index\") == 0)\n",
    "#     )\n",
    "#     .with_columns(\n",
    "#         (pl.col('s').max() - pl.col('s')).alias('s'),\n",
    "#     )\n",
    "#     # .filter(pl.col(\"vehicle_id\") == 886)\n",
    "# )\n",
    "\n",
    "# fig = plot_time_space(\n",
    "#     plot_df,\n",
    "#     hoverdata=\"object_id\",\n",
    "#     vehicle_col=\"object_id\",\n",
    "#     s_col=\"s\",\n",
    "#     markers=True,\n",
    "#     fig=fig,\n",
    "# )\n",
    "\n",
    "# fig.update_layout()\n",
    "\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radar_df[[\"s\", \"s_velocity\", \"d\", \"d_velocity\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.kalman_filter import build_kalman_df\n",
    "\n",
    "filter_df = (\n",
    "    radar_df\n",
    "    # .with_columns((pl.col(\"distanceToFront_s\") + pl.col(\"s\")).alias(\"front_s\"))\n",
    "    .pipe(build_kalman_df, s_col=\"front_s\" if USE_FRONT else \"s\", derive_s_vel=False)\n",
    "    .filter(\n",
    "        pl.col(\"max_time\") < pl.col(\"max_time\").max()  # filter out the outlier times\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMM Filter the Radar Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.group_by(\"vehicle_ind\").agg(pl.col(\"time_ind\").max())[\"time_ind\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filters.vectorized_kalman import batch_imm_df\n",
    "import numpy as np\n",
    "\n",
    "filt_df = batch_imm_df(\n",
    "    filter_df.rename({\"measurement\": \"z\"}),\n",
    "    filters=(\"CALC\", \"CALK\", \"CVLK\"),\n",
    "    M=np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]]),\n",
    "    mu=np.array([0.1, 0.1, 0.8]),\n",
    "    # chunk_size=3_500,\n",
    "    chunk_size=100_000 if not GPU else 10_000 * 1000,\n",
    "    gpu=GPU,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.kalman_filter import join_results\n",
    "\n",
    "joined_df = join_results(filt_df, filter_df, radar_df)\n",
    "\n",
    "if USE_FRONT:\n",
    "    joined_df = joined_df.rename({\"s\": \"front_s\"})\n",
    "\n",
    "joined_df = joined_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the IMM Filter Noise Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.filters.fusion import mahalanobis_distance\n",
    "# from scipy.stats import chi2\n",
    "\n",
    "# target = chi2.ppf(0.95, 4)\n",
    "\n",
    "\n",
    "# test_df = (\n",
    "#     joined_df.lazy()\n",
    "#     .with_columns(\n",
    "#         pl.col(\n",
    "#             \"s\",\n",
    "#             \"s_velocity\",\n",
    "#             \"d\",\n",
    "#             \"d_velocity\",\n",
    "#         )\n",
    "#         .shift(1)\n",
    "#         .backward_fill(1)\n",
    "#         .over(\"object_id\")\n",
    "#         .name.map(lambda x: x + \"_leader\"),\n",
    "#         pl.col(\"P\")\n",
    "#         .cast(pl.List(inner=pl.Float32))\n",
    "#         .shift(1)\n",
    "#         .backward_fill(1)\n",
    "#         .over(\"object_id\")\n",
    "#         .cast(joined_df[\"P\"].dtype)\n",
    "#         .name.map(lambda x: x + \"_leader\"),\n",
    "#     )\n",
    "#     .collect()\n",
    "#     .pipe(\n",
    "#         mahalanobis_distance,\n",
    "#         return_maha=True,\n",
    "#         pos_override=[\"s\"],\n",
    "#         gpu=GPU,\n",
    "#     )\n",
    "# ).select([\"m_sq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "\n",
    "engine_color = [0.65, 0.65, 0.65]\n",
    "gps_color = [0, 0, 0.75]\n",
    "plutron_color = [0, 0, 0]\n",
    "alabama = [165, 30, 54]\n",
    "alabama = [i / 255 for i in alabama]\n",
    "\n",
    "plt.style.use([\"science\", \"ieee\"])\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(6, 2),\n",
    ")\n",
    "\n",
    "# create a kde plot of the d dimension in both directions\n",
    "lane_df = (\n",
    "    joined_df.select(\n",
    "        [\n",
    "            \"lane\",\n",
    "            \"d\",\n",
    "        ]\n",
    "    )\n",
    "    .melt(id_vars=\"lane\", value_name=\"d\")\n",
    "    .filter(pl.col(\"d\").is_between(-6, 6))\n",
    "    .sample(100_000)\n",
    ")\n",
    "\n",
    "\n",
    "lane_df = lane_df.to_pandas()\n",
    "\n",
    "sns.kdeplot(\n",
    "    data=lane_df,\n",
    "    x=\"d\",\n",
    "    hue=\"lane\",\n",
    "    fill=True,\n",
    "    common_norm=False,\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    "    linewidth=1,\n",
    "    palette=[alabama, engine_color],\n",
    ")\n",
    "\n",
    "ax.set_xlim(-2, 6)\n",
    "\n",
    "ax.set_xlabel(\"$d$ (m)\")\n",
    "# remove the title of the legend\n",
    "ax.get_legend().set_title(\"\")\n",
    "\n",
    "# st th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the Lanes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.lane_classification import label_lane, label_lanes_tree\n",
    "from src.gmm.lane_classification import build_lane_model\n",
    "from src.plotting.lane_gmm import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.pipe(\n",
    "    label_lanes_tree,\n",
    "    full_network=full_net,\n",
    "    kalman_network=mainline_net,\n",
    "    lane_width=LANE_WIDTH,\n",
    "    s_col=\"front_s\" if USE_FRONT else \"s\",\n",
    ")\n",
    "\n",
    "\n",
    "print(joined_df.filter(pl.col(\"lane_index\").is_null()).shape[0] / joined_df.shape[0])\n",
    "\n",
    "joined_df = joined_df.with_columns(pl.col(\"lane_index\").fill_null(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Kalman Filtered Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import build_leader_follower_df, add_front_back_s\n",
    "\n",
    "save_df = (\n",
    "    joined_df.select(\n",
    "        [\n",
    "            \"epoch_time\",\n",
    "            \"object_id\",\n",
    "            (pl.col(\"s\") + pl.col(\"distanceToFront_s\")).alias(\"front_s\"),\n",
    "            (pl.col(\"s\") + pl.col(\"distanceToBack_s\")).alias(\"back_s\"),\n",
    "            \"s_velocity\",\n",
    "            \"d\",\n",
    "            \"d_velocity\",\n",
    "            \"lane\",\n",
    "            \"lane_index\",\n",
    "            \"prediction\",\n",
    "            \"missing_data\",\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(((pl.col(\"front_s\") + pl.col(\"back_s\")) / 2).alias(\"centroid_s\"))\n",
    "    .clone()\n",
    ")\n",
    "\n",
    "\n",
    "# add x/y as calculated by the Frenet Transform\n",
    "transformations = [\n",
    "    (\"front_s\", \"d\", \"front_x\", \"front_y\"),\n",
    "    (\"back_s\", \"d\", \"back_x\", \"back_y\"),\n",
    "    (\"centroid_s\", \"d\", \"centroid_x\", \"centroid_y\"),\n",
    "]\n",
    "\n",
    "for s_col, d_col, x_col, y_col in transformations:\n",
    "    save_df = (\n",
    "        save_df.pipe(\n",
    "            mainline_net.frenet2xy,\n",
    "            lane_col=\"lane\",\n",
    "            s_col=s_col,\n",
    "            d_col=d_col,\n",
    "        )\n",
    "        .drop([\"s\", \"angle\"])\n",
    "        .rename({\"x_lane\": x_col, \"y_lane\": y_col})\n",
    "    )\n",
    "\n",
    "\n",
    "save_df.write_parquet(\n",
    "    ROOT.joinpath(\"data\", \"october_kalman_2d_meas.parquet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import (\n",
    "    build_leader_follower_df,\n",
    "    add_front_back_s,\n",
    "    build_leader_follower_entire_history_df,\n",
    ")\n",
    "\n",
    "joined_df = (\n",
    "    joined_df.lazy()\n",
    "    .pipe(\n",
    "        add_front_back_s,\n",
    "        use_global_median=True,\n",
    "    )\n",
    "    .sort(by=[\"epoch_time\"])\n",
    "    .set_sorted([\"epoch_time\"])\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "lf_df = (\n",
    "    joined_df.lazy()\n",
    "    .pipe(\n",
    "        build_leader_follower_entire_history_df,\n",
    "        s_col=\"front_s\" if USE_FRONT else \"s\",\n",
    "        use_lane_index=True,\n",
    "        max_s_gap=0.5 * 25,  # max headway of 0.5 seconds at 25 m/s\n",
    "    )\n",
    "    # .filter(~(pl.col(\"prediction\") & pl.col(\"prediction_leader\")))\n",
    "    # .filter(pl.col(\"lane_index\") <= 1)\n",
    "    .filter((pl.col(\"s_velocity\") > 0.5) & (pl.col(\"s_velocity_leader\") > 0.5))\n",
    "    .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lf_df.filter(\n",
    "# (    pl.col(\"object_id\").is_in(plot_df.filter(pl.col(\"vehicle_id\") == 451)[\"object_id\"])\n",
    "#     & pl.col(\"leader\").is_in(plot_df.filter(pl.col(\"vehicle_id\") == 2809)[\"object_id\"]))\n",
    "\n",
    "# ).select(['s', 'front_s', 'back_s', 's_leader', 'front_s_leader', 'back_s_leader', 'd', 'd_leader', 'association_distance', 'P'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Association Log-Likelihood Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import calc_assoc_liklihood_distance\n",
    "\n",
    "dims = 4\n",
    "\n",
    "lf_df = lf_df.pipe(calc_assoc_liklihood_distance, gpu=GPU, dims=dims, permute=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Match Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (lf_df\n",
    "#     # is this really necessary? Just filters the match for a certain tome\n",
    "#     .pipe(calculate_match_indexes, min_time_threshold=1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "from src.pipelines.association import (\n",
    "    calculate_match_indexes,\n",
    "    pipe_gate_headway_calc,\n",
    "    build_match_df,\n",
    "    # filter_bad_lane_matches,\n",
    ")\n",
    "\n",
    "# prediction_length = 4\n",
    "\n",
    "assoc1_df = (\n",
    "    lf_df\n",
    "    # is this really necessary? Just filters the match for a certain tome\n",
    "    .pipe(calculate_match_indexes, min_time_threshold=1)\n",
    "    .pipe(\n",
    "        pipe_gate_headway_calc,\n",
    "        window=20,\n",
    "        association_dist_cutoff=chi2.ppf(0.95, dims),\n",
    "    )\n",
    "    .collect(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = (\n",
    "    assoc1_df.lazy()\n",
    "    .pipe(\n",
    "        build_match_df,\n",
    "        traj_time_df=joined_df.group_by(\"object_id\")\n",
    "        .agg(\n",
    "            pl.col(\"epoch_time\").max().alias(\"epoch_time_max\"),\n",
    "        )\n",
    "        .lazy(),\n",
    "        assoc_cutoff=chi2.ppf(0.95, dims),\n",
    "        assoc_cutoff_pred=chi2.ppf(0.95, dims),\n",
    "        time_headway_cutoff=0.01,\n",
    "    )\n",
    "    .collect(streaming=True)\n",
    ")\n",
    "\n",
    "\n",
    "# 1 - 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assoc1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Label the Joined Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import create_vehicle_ids\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from networkx.algorithms.connectivity import (\n",
    "    node_connectivity,\n",
    "    all_pairs_node_connectivity,\n",
    ")\n",
    "\n",
    "cc, G, assoc_df = joined_df.pipe(\n",
    "    create_vehicle_ids,\n",
    "    match_df,\n",
    ")\n",
    "\n",
    "\n",
    "def get_ordered_combinations(cc_list):\n",
    "    return [\n",
    "        (int(start), int(end), veh_i) if start < end else (end, start, veh_i)\n",
    "        for veh_i, cc_list in enumerate(cc)\n",
    "        for start, end in combinations(cc_list, 2)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the subgraphs and find those with maximum degree of >= 3\n",
    "need2compute = []\n",
    "for i, c in enumerate(nx.connected_components(G)):\n",
    "    if max(d for n, d in G.subgraph(c).degree) >= 3:\n",
    "        need2compute.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(need2compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need2_filt_ids = (\n",
    "    assoc_df.with_columns(pl.count().over([\"epoch_time\", \"vehicle_id\"]).alias(\"count\"))\n",
    "    .filter(pl.col(\"count\").max().over(\"vehicle_id\") > 2)[\"vehicle_id\"]\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(need2_filt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_end_df = joined_df.group_by(\"object_id\").agg(\n",
    "    pl.col(\"epoch_time\").min().alias(\"begin_time\"),\n",
    "    pl.col(\"epoch_time\").max().alias(\"end_time\"),\n",
    ")\n",
    "\n",
    "combs = get_ordered_combinations(cc)\n",
    "\n",
    "permute_df = (\n",
    "    pl.DataFrame(\n",
    "        combs,\n",
    "        schema={\"start\": pl.UInt64, \"end\": pl.UInt64, \"vehicle_index\": pl.Int64},\n",
    "    )\n",
    "    .filter(pl.col(\"vehicle_index\").is_in(need2_filt_ids))\n",
    "    .join(begin_end_df.rename({\"object_id\": \"start\"}), on=\"start\", how=\"left\")\n",
    "    .join(\n",
    "        begin_end_df.rename({\"object_id\": \"end\"}),\n",
    "        on=\"end\",\n",
    "    )\n",
    "    # filter for overlap in time\n",
    "    .filter(\n",
    "        pl.min_horizontal(pl.col(\"end_time\"), pl.col(\"end_time_right\"))\n",
    "        > pl.max_horizontal(pl.col(\"begin_time\"), pl.col(\"begin_time_right\"))\n",
    "    )\n",
    "    .with_columns(\n",
    "        # sort the object id and leader\n",
    "        pl.when(pl.col(\"start\") < pl.col(\"end\"))\n",
    "        .then(pl.concat_list([pl.col(\"start\"), pl.col(\"end\")]))\n",
    "        .otherwise(pl.concat_list([pl.col(\"end\"), pl.col(\"start\")]))\n",
    "        .alias(\"pair\")\n",
    "    )\n",
    "    .with_columns(pl.col(\"pair\").hash().alias(\"pair_hash\"))\n",
    "    .with_columns(\n",
    "        pl.concat_str(\n",
    "            [pl.col(\"pair\").list.get(0), pl.col(\"pair\").list.get(1)], separator=\"-\"\n",
    "        ).alias(\"pair_str\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.lit(None, dtype=pl.Float64).alias(\"association_distance_filt\"),\n",
    "    )\n",
    "    .join(\n",
    "        match_df.with_columns(\n",
    "            pl.concat_str(\n",
    "                [pl.col(\"pair\").list.get(0), pl.col(\"pair\").list.get(1)], separator=\"-\"\n",
    "            ).alias(\"pair_str\"),\n",
    "            (pl.col(\"prediction\") | pl.col(\"prediction_leader\")).alias(\"prediction\"),\n",
    "        ).select([\"pair_str\", \"prediction\"]),\n",
    "        on=\"pair_str\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(pl.col(\"prediction\").fill_null(False))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_df.filter(pl.col(\"vehicle_index\") == 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "need2compute = permute_df.filter(pl.col(\"association_distance_filt\").is_null()).select(\n",
    "    \"start\", \"end\", \"pair_hash\", \"vehicle_index\", \"prediction\"\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    pl.col(\"object_id\"),\n",
    "    \"epoch_time\",\n",
    "    \"s\",\n",
    "    \"lane\",\n",
    "    \"d\",\n",
    "    \"s_velocity\",\n",
    "    \"d_velocity\",\n",
    "    \"front_s\",\n",
    "    \"back_s\",\n",
    "    \"P\",\n",
    "    \"prediction\",\n",
    "]\n",
    "\n",
    "need2compute = (\n",
    "    need2compute.lazy()\n",
    "    .join(\n",
    "        joined_df.lazy()\n",
    "        # .filter(~(pl.col(\"prediction\")))\n",
    "        .select(cols).rename({\"object_id\": \"start\"}),\n",
    "        on=\"start\",\n",
    "    )\n",
    "    .join(\n",
    "        joined_df.lazy()\n",
    "        # .filter(~(pl.col(\"prediction\")))\n",
    "        # .filter((pl.col(\"lane_index\") < 2))\n",
    "        .select(cols).rename({\"object_id\": \"end\"}),\n",
    "        on=[\"end\", \"epoch_time\"],\n",
    "        suffix=\"_leader\",\n",
    "    )\n",
    "    .filter(\n",
    "        (pl.sum_horizontal(\"prediction\", \"prediction_right\", \"prediction_leader\") == 0)\n",
    "        | (\n",
    "            pl.col(\"prediction\")\n",
    "            & (pl.col(\"prediction_right\") | pl.col(\"prediction_leader\"))\n",
    "        )\n",
    "    )\n",
    "    # .with_columns(\n",
    "    #     (pl.col(\"prediction\") | pl.col(\"prediction_leader\")).alias(\"prediction\"),\n",
    "    #     (\n",
    "    #         (pl.col(\"epoch_time\").max() - pl.col(\"epoch_time\")).dt.total_milliseconds()\n",
    "    #         / 1000\n",
    "    #     )\n",
    "    #     .over(\"pair_hash\")\n",
    "    #     .alias(\"match_time\"),\n",
    "    # )\n",
    "    # .drop(\"prediction_leader\")\n",
    "    # .filter((~pl.col(\"prediction\")) | (pl.col(\"match_time\") <= 0.5))\n",
    "    .collect()\n",
    "    .pipe(calc_assoc_liklihood_distance, gpu=GPU, dims=dims, permute=False)\n",
    "    .lazy()\n",
    "    .with_columns(\n",
    "        pl.col(\"association_distance\")\n",
    "        .rolling_mean(window_size=20, min_periods=1)\n",
    "        .over(\"pair_hash\")\n",
    "    )\n",
    "    .group_by(\"pair_hash\")\n",
    "    .agg(\n",
    "        pl.col(\"association_distance\").mean().alias(\"association_distance_filt\"),\n",
    "        pl.col(\"epoch_time\")\n",
    "        .filter(pl.col(\"association_distance\") < chi2.ppf(0.95, dims))\n",
    "        .first()\n",
    "        .alias(\"join_time\"),\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "\n",
    "permute_df = permute_df.update(need2compute, on=\"pair_hash\", how=\"left\").filter(\n",
    "    pl.col(\"association_distance_filt\").is_not_null()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_G = nx.Graph()\n",
    "\n",
    "for d in permute_df.select([\"start\", \"end\", \"association_distance_filt\"]).to_dicts():\n",
    "    big_G.add_edge(d[\"start\"], d[\"end\"], weight=d[\"association_distance_filt\"])\n",
    "\n",
    "cropped_G = nx.subgraph_view(G, filter_node=lambda x: x in big_G.nodes)\n",
    "\n",
    "bad_edges = nx.difference(big_G, cropped_G).copy()\n",
    "# update the weights from bigG\n",
    "for u, v in bad_edges.edges:\n",
    "    bad_edges[u][v][\"weight\"] = big_G[u][v][\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_df.group_by(pl.col(\"vehicle_index\")).agg(\n",
    "    pl.col(\"association_distance_filt\").max() > chi2.ppf(0.999, dims)\n",
    ").select(pl.count(), pl.col(\"association_distance_filt\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import walk_graph_removals\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_edges = []\n",
    "\n",
    "for veh in tqdm(\n",
    "    permute_df[\n",
    "        # .filter(\n",
    "        #     pl.col(\"association_distance_filt\").mean().over(\"vehicle_index\")\n",
    "        #     > chi2.ppf(0.9999, dims)\n",
    "        # )\n",
    "        \"vehicle_index\"\n",
    "    ].unique()\n",
    "):\n",
    "    # if veh != 1943:\n",
    "    #     continue\n",
    "\n",
    "    remove_edges.extend(\n",
    "        walk_graph_removals(\n",
    "            cropped_G.subgraph(cc[veh]).copy(),\n",
    "            max_removals=20,\n",
    "            cutoff=chi2.ppf(0.999, dims),\n",
    "            # score_func=lambda y: 0.1\n",
    "            # df=permute_df.filter(pl.col(\"vehicle_index\") == veh),\n",
    "            big_G=big_G,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2.ppf(0.9999, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_df.filter(\n",
    "    (\n",
    "        (pl.col(\"association_distance_filt\") > chi2.ppf(0.99, dims))\n",
    "        .sum()\n",
    "        .over(\"vehicle_index\")\n",
    "        == 2\n",
    "    )\n",
    "    & (pl.count().over(\"vehicle_index\") <= 5)\n",
    ")[\"vehicle_index\"].sample(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import get_graph_score\n",
    "from src.plotting.single_veh import plot_graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "engine_color = [0.65, 0.65, 0.65]\n",
    "gps_color = [0, 0, 0.75]\n",
    "plutron_color = [0, 0, 0]\n",
    "# alabama = [165, 30, 54]\n",
    "# alabama = [i / 255 for i in alabama]\n",
    "\n",
    "alabama = \"#%02x%02x%02x\" % (165, 30, 54)\n",
    "\n",
    "\n",
    "vehicle_id = 140  # use sequence 15 for the example\n",
    "# vehicle_id = 302\n",
    "nodes = cc[vehicle_id]\n",
    "\n",
    "sb_graph = cropped_G.subgraph(nodes)\n",
    "bad_graph = nx.subgraph_view(bad_edges, filter_node=lambda x: x in sb_graph.nodes)\n",
    "score = get_graph_score(sub_graph=sb_graph, big_G=big_G, cutoff=chi2.ppf(0.9875, dims))\n",
    "\n",
    "# make two side by side plots using\n",
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(6, 3))\n",
    "\n",
    "# plot the small graph\n",
    "print(score)\n",
    "node_num_map = plot_graph(\n",
    "    subgraph=sb_graph,\n",
    "    full_graph=big_G,\n",
    "    ax=ax[0],\n",
    "    vehicle_colors=[engine_color, gps_color, plutron_color],\n",
    "    bad_edge_color=alabama,\n",
    ")\n",
    "\n",
    "from src.pipelines.association import walk_graph_removals\n",
    "\n",
    "removal_edges = walk_graph_removals(\n",
    "    sb_graph.copy(), cutoff=chi2.ppf(0.99, dims), max_removals=100, big_G=big_G\n",
    ")\n",
    "\n",
    "p_graph = cropped_G.subgraph(nodes).copy()\n",
    "for edge in removal_edges:\n",
    "    p_graph.remove_edge(*edge)\n",
    "plot_graph(\n",
    "    subgraph=p_graph,\n",
    "    full_graph=big_G,\n",
    "    ax=ax[1],\n",
    "    vehicle_colors=list(\n",
    "        map(\n",
    "            lambda x: [\n",
    "                x,\n",
    "            ],\n",
    "            [engine_color, gps_color, plutron_color],\n",
    "        )\n",
    "    ),\n",
    "    bad_edge_color=[alabama],\n",
    "    node_num_map=node_num_map,\n",
    ")\n",
    "\n",
    "# remove the ticks\n",
    "for a in ax[:-1]:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "\n",
    "\n",
    "# get a 10 minute window\n",
    "plot_df = (\n",
    "    joined_df.pipe(radar_obj.add_cst_timezone)\n",
    "    .join(\n",
    "        pl.DataFrame(\n",
    "            [\n",
    "                (\n",
    "                    i,\n",
    "                    v,\n",
    "                )\n",
    "                for i, v_list in enumerate(list(nx.connected_components(p_graph)))\n",
    "                for v in v_list\n",
    "            ],\n",
    "            schema={\n",
    "                \"vehicle_id_new\": int,\n",
    "                \"object_id\": pl.UInt64,\n",
    "            },\n",
    "        ),\n",
    "        on=\"object_id\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"epoch_time\") - pl.col(\"epoch_time\").min())\n",
    "        .dt.total_seconds()\n",
    "        .alias(\"dt\"),\n",
    "    )\n",
    "    .join(\n",
    "        pl.DataFrame(\n",
    "            list(node_num_map.items()),\n",
    "            schema={\"object_id\": pl.UInt64, \"obj_id_clean\": int},\n",
    "        ),\n",
    "        on=\"object_id\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# plot the trajectories\n",
    "plot_df = plot_df.to_pandas()\n",
    "\n",
    "# for _df in plot_df.groupby('vehicle_id_new'):\n",
    "\n",
    "sns.lineplot(\n",
    "    data=plot_df,\n",
    "    x=\"dt\",\n",
    "    y=\"front_s\",\n",
    "    hue=\"vehicle_id_new\",\n",
    "    ax=ax[-1],\n",
    "    legend=False,\n",
    "    palette=[engine_color, gps_color, plutron_color],\n",
    ")\n",
    "\n",
    "# plot the text over the line\n",
    "for _id, _df in plot_df.groupby(\"obj_id_clean\"):\n",
    "    # add the text over the plot\n",
    "    ax[-1].annotate(\n",
    "        xy=(_df[\"dt\"].mean(), _df[\"front_s\"].mean()),\n",
    "        text=_id,\n",
    "        **{\"fontsize\": 10},\n",
    "    )\n",
    "\n",
    "\n",
    "# change the labels\n",
    "ax[-1].set_xlabel(\"Time (s)\")\n",
    "ax[-1].set_ylabel(\"$s$ (m)\")\n",
    "\n",
    "# drop most of the x ticks\n",
    "# ax[0].set_xticks(ax[0].get_xticks()[::1])d\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from src.plotting.time_space import plot_time_space\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# get a 10 minute window\n",
    "plot_df = (\n",
    "    assoc_df\n",
    "    # .with_columns(pl.col(\"\").alias(\"s\"))\n",
    "    .pipe(radar_obj.add_cst_timezone).filter(\n",
    "        pl.col(\"epoch_time_cst\").is_between(\n",
    "            pl.lit(\"2023-10-31 12:13:03\").str.strptime(\n",
    "                pl.Datetime(time_unit=\"ns\", time_zone=\"US/Central\"),\n",
    "            ),\n",
    "            pl.lit(\"2023-10-31 12:14:18\").str.strptime(\n",
    "                pl.Datetime(time_unit=\"ns\", time_zone=\"US/Central\"),\n",
    "            ),\n",
    "        )\n",
    "        & (pl.col(\"lane\").str.contains(\"W\"))\n",
    "        & (pl.col(\"lane_index\") == 0)\n",
    "        # & (pl.col('vehicle_id') == 1927)\n",
    "    )\n",
    "    # .with_columns(\n",
    "    #     (pl.col(\"front_s\").max() - pl.col(\"front_s\")).alias(\"front_s\"),\n",
    "    # )\n",
    "    # .filter(pl.col(\"vehicle_id\") == 886)\n",
    ")\n",
    "\n",
    "fig = plot_time_space(\n",
    "    plot_df,\n",
    "    hoverdata=\"object_id\",\n",
    "    vehicle_col=\"vehicle_id\",\n",
    "    s_col=\"front_s\",\n",
    "    markers=True,\n",
    "    fig=fig,\n",
    "    every=1,\n",
    ")\n",
    "\n",
    "# fig.update_layout(yaxis_autorange=\"reversed\")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import make_graph_based_ids\n",
    "\n",
    "mainG = G.copy()\n",
    "\n",
    "# remove_edges = correction_df[\"remove_edges\"].to_list()\n",
    "\n",
    "for edge in remove_edges:\n",
    "    mainG.remove_edge(*edge)\n",
    "\n",
    "new_assoc_df = make_graph_based_ids(assoc_df.drop(\"vehicle_id\"), mainG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from src.plotting.time_space import plot_time_space\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# get a 10 minute window\n",
    "plot_df = (\n",
    "    new_assoc_df\n",
    "    # .with_columns(pl.col(\"\").alias(\"s\"))\n",
    "    .pipe(radar_obj.add_cst_timezone).filter(\n",
    "        pl.col(\"epoch_time_cst\").is_between(\n",
    "            pl.lit(\"2023-10-31 12:13:03\").str.strptime(\n",
    "                pl.Datetime(time_unit=\"ns\", time_zone=\"US/Central\"),\n",
    "            ),\n",
    "            pl.lit(\"2023-10-31 12:14:18\").str.strptime(\n",
    "                pl.Datetime(time_unit=\"ns\", time_zone=\"US/Central\"),\n",
    "            ),\n",
    "        )\n",
    "        & (pl.col(\"lane\").str.contains(\"W\"))\n",
    "        & (pl.col(\"lane_index\") == 0)\n",
    "        # & (pl.col('vehicle_id') == 1927)\n",
    "    )\n",
    "    # .with_columns(\n",
    "    #     (pl.col(\"front_s\").max() - pl.col(\"front_s\")).alias(\"front_s\"),\n",
    "    # )\n",
    "    # .filter(pl.col(\"vehicle_id\") == 886)\n",
    ")\n",
    "\n",
    "fig = plot_time_space(\n",
    "    plot_df,\n",
    "    hoverdata=\"object_id\",\n",
    "    vehicle_col=\"vehicle_id\",\n",
    "    s_col=\"front_s\",\n",
    "    markers=True,\n",
    "    fig=fig,\n",
    "    every=1,\n",
    ")\n",
    "\n",
    "# fig.update_layout(yaxis_autorange=\"reversed\")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_assoc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuse & Smooth the Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.association import build_fusion_df\n",
    "\n",
    "fusion_df = (\n",
    "    new_assoc_df.pipe(\n",
    "        build_fusion_df, prediction_length=prediction_length, max_vehicle_num=3\n",
    "    )\n",
    "    .collect(streaming=True)\n",
    "    .pipe(\n",
    "        radar_obj.add_cst_timezone,\n",
    "        time_col=\"epoch_time\",\n",
    "    )\n",
    ")\n",
    "# fusion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion_df[['s', 'front_s', 'back_s']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_adjuster = (\n",
    "    fusion_df.group_by(\n",
    "        [\n",
    "            \"vehicle_id\",\n",
    "            \"time_index\",\n",
    "        ]\n",
    "    )\n",
    "    .agg(\n",
    "        pl.col(\"distanceToFront_s\")\n",
    "        .filter(pl.col(\"approaching\"))\n",
    "        .mean()\n",
    "        .alias(\"distanceToFront_s\"),\n",
    "        pl.col(\"distanceToBack_s\")\n",
    "        .filter(~pl.col(\"approaching\"))\n",
    "        .mean()\n",
    "        .alias(\"distanceToBack_s\"),\n",
    "        pl.col(\"distanceToFront_s\").mean().alias(\"distanceToFront_s_all\"),\n",
    "        pl.col(\"distanceToBack_s\").mean().alias(\"distanceToBack_s_all\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"distanceToFront_s\").is_null())\n",
    "        .then(pl.col(\"distanceToFront_s_all\"))\n",
    "        .otherwise(pl.col(\"distanceToFront_s\"))\n",
    "        .alias(\"distanceToFront_s\"),\n",
    "        pl.when(pl.col(\"distanceToBack_s\").is_null())\n",
    "        .then(pl.col(\"distanceToBack_s_all\"))\n",
    "        .otherwise(pl.col(\"distanceToBack_s\"))\n",
    "        .alias(\"distanceToBack_s\"),\n",
    "    )\n",
    "    .drop([\"distanceToFront_s_all\", \"distanceToBack_s_all\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_df = (\n",
    "    fusion_df.select(\n",
    "        [\n",
    "            \"front_s\",\n",
    "            \"s\",\n",
    "            \"back_s\",\n",
    "            \"s_velocity\",\n",
    "            \"d\",\n",
    "            \"d_velocity\",\n",
    "            \"P\",\n",
    "            \"vehicle_id\",\n",
    "            \"time_index\",\n",
    "            \"vehicle_time_index_int\",\n",
    "            \"prediction\",\n",
    "            \"length_s\",\n",
    "        ]\n",
    "    )\n",
    "    .lazy()\n",
    "    .filter(~pl.col(\"prediction\"))\n",
    ")\n",
    "\n",
    "outer_df = (\n",
    "    (\n",
    "        outer_df.join(\n",
    "            outer_df,\n",
    "            on=[\"time_index\", \"vehicle_id\"],\n",
    "            how=\"outer\",\n",
    "            suffix=\"_leader\",\n",
    "        )\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.struct(\n",
    "            [\n",
    "                pl.min_horizontal(\n",
    "                    [\n",
    "                        pl.col(\"vehicle_time_index_int\"),\n",
    "                        pl.col(\"vehicle_time_index_int_leader\"),\n",
    "                    ]\n",
    "                ).alias(\"one\"),\n",
    "                pl.max_horizontal(\n",
    "                    [\n",
    "                        pl.col(\"vehicle_time_index_int\"),\n",
    "                        pl.col(\"vehicle_time_index_int_leader\"),\n",
    "                    ]\n",
    "                ).alias(\"two\"),\n",
    "            ]\n",
    "        ).alias(\"vehicle_time_struct\"),\n",
    "    )\n",
    "    # .collect()\n",
    "    .filter(\n",
    "        (\n",
    "            pl.col(\"vehicle_time_index_int\")\n",
    "            .cum_count()\n",
    "            .over([\"vehicle_id\", \"time_index\", \"vehicle_time_struct\"])\n",
    "            < 1\n",
    "        )\n",
    "        & (pl.col(\"vehicle_time_index_int\") != pl.col(\"vehicle_time_index_int_leader\"))\n",
    "    )\n",
    "    .collect(streaming=True)\n",
    "    .pipe(\n",
    "        calc_assoc_liklihood_distance,\n",
    "        gpu=GPU,\n",
    "        dims=4,\n",
    "    )\n",
    "    .group_by([\"vehicle_id\", \"time_index\"])\n",
    "    .agg(pl.col(\"association_distance\").max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tracker = (\n",
    "    fusion_df.select(\n",
    "        [\n",
    "            \"vehicle_id\",\n",
    "            \"time_index\",\n",
    "            # \"vehicle_time_index_int\",\n",
    "            \"prediction\",\n",
    "        ]\n",
    "    )\n",
    "    .lazy()\n",
    "    .group_by([\"vehicle_id\", \"time_index\"])\n",
    "    .agg(pl.col(\"prediction\").all().alias(\"prediction\"))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_df.select(\n",
    "    (pl.col(\"distanceToFront_s\").max() - pl.col(\"distanceToFront_s\").min()).over(\n",
    "        \"vehicle_id\"\n",
    "    )\n",
    ")[\"distanceToFront_s\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filters.fusion import batch_join, rts_smooth\n",
    "\n",
    "GPU = True\n",
    "\n",
    "merged_df = batch_join(\n",
    "    fusion_df,\n",
    "    # .with_columns(\n",
    "    #     pl.col(\"back_s\").min().over([\"vehicle_id\", \"time_index\"]).alias(\"back_s_min\"),\n",
    "    #     pl.col(\"front_s\").max().over([\"vehicle_id\", \"time_index\"]).alias(\"front_s_max\"),\n",
    "    # ).with_columns(((pl.col(\"back_s_min\") + pl.col(\"front_s_max\")) / 2).alias(\"s\")),\n",
    "    method=\"ImprovedFastCI\",\n",
    "    batch_size=10_000 if not GPU else 10_000,\n",
    "    gpu=GPU,\n",
    "    s_col=\"front_s\" if USE_FRONT else \"s\",\n",
    ")\n",
    "merged_df = rts_smooth(\n",
    "    merged_df,\n",
    "    batch_size=10_000 if not GPU else 50_000,\n",
    "    gpu=GPU,\n",
    "    s_col=\"front_s\" if USE_FRONT else \"s\",\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(\"lane_index\").pipe(\n",
    "    label_lanes_tree,\n",
    "    full_network=full_net,\n",
    "    kalman_network=mainline_net,\n",
    "    lane_width=LANE_WIDTH,\n",
    "    s_col=\"s_smooth\",\n",
    "    d_col=\"d_smooth\",\n",
    ")\n",
    "\n",
    "\n",
    "merged_df = (\n",
    "    merged_df.lazy()\n",
    "    .join(\n",
    "        fusion_df.lazy()\n",
    "        .select([\"time_index\", \"vehicle_id\", \"object_id\", \"length_s\"])\n",
    "        .group_by([\"time_index\", \"vehicle_id\"])\n",
    "        .agg(pl.col(\"object_id\"), pl.col(\"length_s\").mean()),\n",
    "        on=[\"time_index\", \"vehicle_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .join(\n",
    "        outer_df.lazy().select([\"time_index\", \"vehicle_id\", \"association_distance\"]),\n",
    "        on=[\"time_index\", \"vehicle_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .join(\n",
    "        distance_adjuster.lazy(),\n",
    "        on=[\"vehicle_id\", \"time_index\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .join(\n",
    "        prediction_tracker.lazy(),\n",
    "        on=[\"vehicle_id\", \"time_index\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col(\"ci_s\") + pl.col(\"distanceToFront_s\")).alias(\"ci_front_s\"),\n",
    "        (pl.col(\"ci_s\") + pl.col(\"distanceToBack_s\")).alias(\"ci_back_s\"),\n",
    "        (pl.col(\"s_smooth\") + pl.col(\"distanceToFront_s\")).alias(\"front_s_smooth\"),\n",
    "        (pl.col(\"s_smooth\") + pl.col(\"distanceToBack_s\")).alias(\"back_s_smooth\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        ((pl.col(\"ci_front_s\") + pl.col(\"ci_back_s\")) / 2).alias(\"ci_s\"),\n",
    "        ((pl.col(\"front_s_smooth\") + pl.col(\"back_s_smooth\")) / 2).alias(\"s_smooth\"),\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.plotting.single_veh import plot_vehicle\n",
    "\n",
    "vehs = [160]\n",
    "\n",
    "fig = None\n",
    "\n",
    "for veh in vehs:\n",
    "    ind_vehicles = fusion_df.filter(pl.col(\"vehicle_id\") == veh)[\"object_id\"].unique()\n",
    "\n",
    "    fig = plot_vehicle(\n",
    "        fusion_df.filter(pl.col(\"vehicle_id\") == veh).pipe(radar_obj.add_cst_timezone),\n",
    "        s_velocity_col=\"s_velocity\",\n",
    "        s_col=\"front_s\",\n",
    "        fig=fig,\n",
    "        data_name=\"IMM Filtered\",\n",
    "        # show_d=False\n",
    "    )\n",
    "\n",
    "    plot_vehicle(\n",
    "        merged_df.filter(pl.col(\"vehicle_id\") == veh).pipe(radar_obj.add_cst_timezone),\n",
    "        # s_col=\"s_smooth\",\n",
    "        s_col=\"front_s_smooth\",\n",
    "        s_velocity_col=\"s_velocity_smooth\",\n",
    "        # s_velocity_col=\"ci_s_velocity\",\n",
    "        d_col=\"d_smooth\",\n",
    "        d_velocity_col=\"d_velocity_smooth\",\n",
    "        fig=fig,\n",
    "        color=\"blue\",\n",
    "        data_name=\"Fused and Smoothed\",\n",
    "        # show_d=False\n",
    "    )\n",
    "\n",
    "    fig = plot_vehicle(\n",
    "        merged_df.filter(pl.col(\"vehicle_id\") == veh).pipe(radar_obj.add_cst_timezone),\n",
    "        # s_col=\"s_smooth\",\n",
    "        s_col=\"ci_front_s\",\n",
    "        s_velocity_col=\"ci_s_velocity\",\n",
    "        # s_velocity_col=\"ci_s_velocity\",\n",
    "        d_col=\"ci_d\",\n",
    "        d_velocity_col=\"ci_d_velocity\",\n",
    "        fig=fig,\n",
    "        color=\"purple\",\n",
    "        data_name=\"CI Fused\",\n",
    "        # show_d=False\n",
    "    )\n",
    "\n",
    "    plot_vehicle(\n",
    "        radar_df.filter(\n",
    "            pl.col(\"object_id\").is_in(\n",
    "                new_assoc_df.filter(pl.col(\"vehicle_id\") == veh)[\"object_id\"]\n",
    "            )\n",
    "            & ~pl.col(\"prediction\")\n",
    "        ).pipe(\n",
    "            radar_obj.add_cst_timezone,\n",
    "        ),\n",
    "        # s_col=\"back_s\",\n",
    "        s_velocity_col=\"f32_velocityInDir_mps\",\n",
    "        d_col=\"d\",\n",
    "        # d_velocity_col=\"d_velocity\",\n",
    "        fig=fig,\n",
    "        color=\"black\",\n",
    "        data_name=\"Radar Raw\",\n",
    "        # show_d=False\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    # use journal paper font\n",
    "    font_family=\"Times New Roman\",\n",
    "    # set the font size\n",
    "    font_size=22,\n",
    "    # update the subplot titles\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_size=44,\n",
    "    # set the x_limit\n",
    "    yaxis=dict(\n",
    "        title=\"S Distance [m]\",\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"S Velocity [m/s]\",\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        title=\"D Distance [m/s]\",\n",
    "    ),\n",
    "    yaxis4=dict(\n",
    "        title=\"D Velocity [m/s]\",\n",
    "    ),\n",
    "    # set the size of the figure\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    template=\"ggplot2\",\n",
    "    # set the margin to reduce the whitespace\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    # put the legend on the bottom\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "        font=dict(\n",
    "            # family=\"Times New Roman\",\n",
    "            size=22,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap Lane after Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filters.vectorized_kalman import CALCFilter\n",
    "\n",
    "print(CALCFilter.w_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add x/y as calculated by the Frenet Transform\n",
    "transformations = [\n",
    "    (\"front_s_smooth\", \"d_smooth\", \"front_x_smooth\", \"front_y_smooth\"),\n",
    "    (\"back_s_smooth\", \"d_smooth\", \"back_x_smooth\", \"back_y_smooth\"),\n",
    "    (\"s_smooth\", \"d_smooth\", \"centroid_x_smooth\", \"centroid_y_smooth\"),\n",
    "    (\"ci_front_s\", \"ci_d\", \"ci_front_x\", \"ci_front_y\"),\n",
    "    (\"ci_back_s\", \"ci_d\", \"ci_back_x\", \"ci_back_y\"),\n",
    "    (\"ci_s\", \"ci_d\", \"ci_centroid_x\", \"ci_centroid_y\"),\n",
    "]\n",
    "\n",
    "for s_col, d_col, x_col, y_col in transformations:\n",
    "    merged_df = (\n",
    "        merged_df.drop([x_col, y_col])\n",
    "        .pipe(\n",
    "            mainline_net.frenet2xy,\n",
    "            lane_col=\"lane\",\n",
    "            s_col=s_col,\n",
    "            d_col=d_col,\n",
    "        )\n",
    "        .drop([\"s\", \"angle\"])\n",
    "        .rename({\"x_lane\": x_col, \"y_lane\": y_col})\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(*[c for c in merged_df.columns if (\"_int\" in c)], \"ci_P\").write_parquet(\n",
    "    ROOT / \"data\" / \"merged_october_2_meas.parquet\",\n",
    "    use_pyarrow=True,\n",
    "    compression=\"zstd\",\n",
    "    compression_level=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.filter(pl.col(\"vehicle_id\").is_in([4298, 4313])).drop(\n",
    "    *[c for c in merged_df.columns if (\"_int\" in c)], \"ci_P\"\n",
    ").write_parquet(\n",
    "    ROOT / \"data\" / \"traj_samples.parquet\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar-trajectories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
